#!/bin/bash

#set -x

if [ "$(arch)" != "x86_64" ]; then
    echo "This script only supports x86_64 architecture at the moment."
    exit 1
fi

cluster_delete=
install_dashboard=
helm_name=jarvice
helm_namespace=jarvice-system
database_disk_delete=
vault_disks_delete=

jarvice_chart_dir=$(realpath $(dirname $0)/..)
config_file=$(realpath $(dirname $0)/gke-cluster.yaml)

function jarvice_deploy2usage {
    cat <<EOF
Usage:
  $0 [global_options] [deploy_or_delete_options]

Available [global_options]:
  --jarvice-chart-dir <path>        Alternative JARVICE helm chart directory
                                    (Default: $jarvice_chart_dir)
  --config-file <filename>          Alternative cluster config file
                                    (Default: $config_file)

Available [delete_options]:
  --cluster-delete                  Delete the cluster
  --database-disk-delete            Delete the database disk on cluster delete
  --vault-disks-delete              Delete the vault disks on cluster delete

EOF
}
#Available [deploy_options]:
#  --install-dashboard               Install kubernetes dashboard into cluster


while [ $# -gt 0 ]; do
    case $1 in
        --help)
            jarvice_deploy2usage
            exit 0
            ;;
        --jarvice-chart-dir)
            jarvice_chart_dir=$2
            shift; shift
            ;;
        --config-file)
            config_file=$2
            shift; shift
            ;;
        --install-dashboard)
            install_dashboard=y
            shift
            ;;
        --cluster-delete)
            cluster_delete=y
            shift
            ;;
        --database-disk-delete)
            database_disk_delete=y
            shift
            ;;
        --vault-disks-delete)
            vault_disks_delete=y
            shift
            ;;
        *)
            jarvice_deploy2usage
            exit 1
            ;;
    esac
done

echo
echo "* Using cluster configuration file $config_file..."
if [ ! -f "$config_file" ]; then
    echo "Cannot find cluster config file $config_file.  Cannot continue."
    exit 1
fi

# TODO:
# If NVIDIA node type found, auto install NVIDIA plugin

KUBECTL=$(type -p kubectl)
GCLOUD=$(type -p gcloud)
HELM=$(type -p helm)
CURL=$(type -p curl)
UNZIP=$(type -p unzip)
JQ=$(type -p jq)
SHYAML=$(type -p shyaml)

KUBECTL_VER_REQ=1.10

if [ -z "$KUBECONFIG" ]; then
    KUBECONFIG=~/.kube/config
fi
export KUBECONFIG

###############################################################################

function check_deps {
    if [ -z "$KUBECTL" -o -z "$GCLOUD" -o -z "$HELM" -o -z "$JQ" ]; then
        echo "Missing software needs to be installed.  Verifying sudo access..."
        SUID=$(sudo id -u)
        if [ "$SUID" != "0" ]; then
            echo "Could not verify sudo access.  Cannot continue."
            echo "Please resolve sudo access before re-running this script."
            exit 1
        else
            echo "Verified sudo access..."
        fi
        install_pkgs=
        if [ -z "$CURL" ]; then
            echo "curl command not found...  Installing..."
            install_pkgs+="curl "
        fi
        if [ -z "$JQ" ]; then
            echo "jq command not found...  Installing..."
            install_pkgs+="jq "
        fi
        if [ -z "$SHYAML" ]; then
            echo "shyaml command not found...  Installing..."
            sudo -H pip install shyaml
        fi
        if [ -n "$install_pkgs" ]; then
            if [ -e /etc/redhat-release ]; then
                sudo yum -y install $install_pkgs
            else
                sudo apt-get -y update
                sudo apt-get -y install $install_pkgs
            fi
        fi
    fi
}

function get_json_value {
    json=$1
    key=$2
    echo "$json" | \
        python -c "import json,sys;obj=json.load(sys.stdin);print obj$key;" \
        2>/dev/null || /bin/true
}

function get_yaml_value {
    key=$1
    cat $config_file | \
        $SHYAML -q get-value $key | grep -v ^None || /bin/true
}

function strict_version {
    ver=$1
    ver_req=$2
    python -c "from distutils.version import StrictVersion;print StrictVersion('$ver') >= StrictVersion('$ver_req');"
}


###############################################################################

# gcloud flags
gcloud_global_flags=$(cat $config_file | \
                $SHYAML get-values gcloud.global_flags 2>/dev/null | \
                sed -r 's/None//g')
gcloud_create_flags=$(cat $config_file | \
                $SHYAML get-values gcloud.create_flags 2>/dev/null | \
                sed -r 's/None//g')

# Cluster config options
cluster_name=$(get_yaml_value "gcloud.metadata.name")
cluster_zone=$(get_yaml_value "gcloud.create_flags.--zone")
#aws_region=$(get_yaml_value "metadata.region")

# JARVICE config options
registry_username=$(get_yaml_value "jarvice.imagePullSecret.username")
registry_password=$(get_yaml_value "jarvice.imagePullSecret.password")
jarvice_license=$(get_yaml_value "jarvice.JARVICE_LICENSE_LIC")
jarvice_username=$(get_yaml_value "jarvice.JARVICE_REMOTE_USER")
jarvice_apikey=$(get_yaml_value "jarvice.JARVICE_REMOTE_APIKEY")

if [ -n "$cluster_delete" ]; then
    set -e
    check_deps

    echo
    echo "* Using $KUBECONFIG for kubeconfig..."

    echo
    echo "* Starting deletion of cluster '$cluster_name' in zone '$cluster_zone'..."

    if [ -n "$vault_disks_delete" ]; then
        echo; echo "* Deleting user vault disks..."
        $KUBECTL -n $helm_namespace-jobs delete pods --all
        $KUBECTL -n $helm_namespace-jobs delete pvc --all
    else
        echo; echo "* Preserving user vault disks..."
    fi

    if [ -n "$database_disk_delete" ]; then
        echo
        echo "* Deleting disk containing JARVICE database..."
        $KUBECTL -n $helm_namespace delete deployment jarvice-db
        $KUBECTL -n $helm_namespace delete pvc jarvice-db-pvc
    else
        echo; echo "* Preserving disk containing the JARVICE database..."
    fi

    echo
    echo "* Deleting cluster '$cluster_name' in zone '$cluster_zone'..."
    $GCLOUD container clusters delete --quiet $gcloud_global_flags $cluster_name

    echo
    echo "------------------------------------------------------------------------"
    echo
    echo "Succesfully deleted '$cluster_name' cluster is in the '$cluster_zone' zone."
    if [ -z "$database_disk_delete" -o -z "$vault_disks_delete" ]; then
        echo
        echo "** Preserved disks will be reused if another '$cluster_name' cluster is created in the '$cluster_zone' zone."
    fi
    echo
    exit 0
fi

if [ -z "$registry_username" -o -z "$registry_password" -o -z "$jarvice_license" -o -z "$jarvice_username" -o -z "$jarvice_apikey" ]; then
    echo "Missing credentials and/or license.  Cannot continue."
    echo "Please update the configuration file: $config_file"
    jarvice_deploy2usage
fi

# Assumes helm chart basedir is one level up from this script
echo "Verifying helm chart directory '$jarvice_chart_dir'..."
CHART_NAME=$(grep '^name: jarvice' $jarvice_chart_dir/Chart.yaml 2>/dev/null)
if [ -z "$CHART_NAME" ]; then
    echo "Could not verify helm chart for JARVICE.  Cannot continue."
    echo "Use --jarvice-chart-dir to specify valid JARVICE helm chart" \
        "directory."
    exit 1
else
    echo "Found valid chart directory..."
fi

set -e
check_deps

[ -z "$INSTALL_DIR" ] && INSTALL_DIR=/usr/local

if [ -z "$KUBECTL" ]; then
    KUBECTL=$INSTALL_DIR/bin/kubectl
    echo "kubectl command not found...  Installing to $KUBECTL..."
    sudo bash -c "curl --silent --location https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl >$KUBECTL"
    sudo chown root.root $KUBECTL
    sudo chmod 755 $KUBECTL
else
    KUBECTL_MAJOR=$($KUBECTL version | grep '^Client' | sed -E 's/.*Major:"([0-9]+)".*/\1/')
    KUBECTL_MINOR=$($KUBECTL version | grep '^Client' | sed -E 's/.*Minor:"([0-9]+)\+*".*/\1/')
    KUBECTL_VER="$KUBECTL_MAJOR.$KUBECTL_MINOR"
    echo "Found $KUBECTL... Version: $KUBECTL_VER"
    if [ "$(strict_version $KUBECTL_VER $KUBECTL_VER_REQ)" != "True" ]; then
        echo "kubectl version $KUBECTL_VER_REQ or newer is required."
        echo "Please upgrade kubectl or remove it."
        echo "This script will re-install kubectl if it is removed."
        exit 1
    fi
fi

if [ -z "$GCLOUD" ]; then
    echo "gcloud command not found...  Installing..."
    if [ -e /etc/redhat-release ]; then
        cat <<EOF | sudo tee /etc/yum.repos.d/google-cloud-sdk.repo >/dev/null
[google-cloud-sdk]
name=Google Cloud SDK
baseurl=https://packages.cloud.google.com/yum/repos/cloud-sdk-el7-x86_64
enabled=1
gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg
       https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF
        sudo yum -y install google-cloud-sdk
    else
        export CLOUD_SDK_REPO="cloud-sdk-$(lsb_release -c -s)"
        echo "deb http://packages.cloud.google.com/apt $CLOUD_SDK_REPO main" | \
            sudo tee /etc/apt/sources.list.d/google-cloud-sdk.list >/dev/null
        curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | \
            sudo apt-key add -
        sudo apt-get -y update && sudo apt-get -y install google-cloud-sdk
    fi
    echo "Running 'gcloud init'..."
    gcloud init
fi

if [ -z "$HELM" ]; then
    HELM_INSTALL_DIR=$INSTALL_DIR/bin
    HELM=$HELM_INSTALL_DIR/helm
    echo "helm command not found...  Installing to $HELM..."
    curl --silent https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get >/tmp/get_helm.sh
    sudo --preserve-env=PATH,HELM_INSTALL_DIR bash /tmp/get_helm.sh >/dev/null
    rm -f /tmp/get_helm.sh
else
    echo "Found $HELM..."
fi

echo "------------------------------------------------------------------------"

echo
echo "* Using $KUBECONFIG for kubeconfig..."
OLD_CONTEXT=$($KUBECTL config current-context 2>/dev/null || /bin/true)

echo
echo "* Running $GCLOUD to create cluster..."
    #--no-enable-basic-auth \
    #--enable-ip-alias \
    #--issue-client-certificate \
$GCLOUD container clusters create $cluster_name \
    --labels jarvice-cluster=$cluster_name \
    --scopes gke-default,compute-rw,storage-ro \
    --metadata disable-legacy-endpoints=true \
    --image-type UBUNTU \
    --min-cpu-platform "Intel Skylake" \
    $gcloud_global_flags $gcloud_create_flags

node_pools=$(cat $config_file | $SHYAML keys gcloud.node_pools)
for pool in $node_pools; do
    echo "* Adding node pool '$pool'..."
    node_pool_flags=$(cat $config_file | \
                $SHYAML get-values gcloud.node_pools.$pool 2>/dev/null | \
                sed -r 's/None//g')
    $GCLOUD container node-pools create $pool \
        --scopes gke-default,compute-rw,storage-ro \
        --metadata disable-legacy-endpoints=true \
        --image-type UBUNTU \
        --min-cpu-platform "Intel Skylake" \
        $gcloud_global_flags $node_pool_flags --cluster $cluster_name
done

echo
echo "* Running NVIDIA driver installer..."
$KUBECTL apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/ubuntu/daemonset-preloaded.yaml

echo
echo "* Waiting for kubernetes master to come back online..."
# Strangely, the master does not go offline immediately, so a sleep is needed
[ -n "$node_pools" ] && sleep 90
set +e
while ! $KUBECTL get nodes >/dev/null 2>&1; do
    echo -n "."
    sleep 5
done
set -e
echo; echo
echo "* Cluster nodes are ready..."
$KUBECTL get nodes

echo
echo "* Updating node taints..."
# GKE auto-adds taint for NVIDIA nodes.  We don't want that.  It's in the way.
$KUBECTL taint nodes --all=true nvidia.com/gpu- || /bin/true
roles="jarvice-system jarvice-dockerbuild jarvice-dockerpull jarvice-compute"
for role in $roles; do
    $KUBECTL taint nodes -l node-role.kubernetes.io/$role= \
        node-role.kubernetes.io/$role=:NoSchedule
done

#if [ -n "$install_weave_plugin" ]; then
#    echo
#    echo "* Installing Weave Net network plugin into cluster..."
#    $KUBECTL apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
#fi

JARVICE_MACHINES_ADD='[{"mc_name":"n0", "mc_description":"2 core, 16GB RAM (CPU only)", "mc_cores":"2", "mc_slots":"16", "mc_gpus":"0", "mc_ram":"16", "mc_swap":"8", "mc_scratch":"64", "mc_devices":"", "mc_properties":"node-role.kubernetes.io/jarvice-compute=", "mc_slave_properties":"", "mc_slave_gpus":"0", "mc_slave_ram":"16", "mc_scale_min":"1", "mc_scale_max":"1", "mc_scale_select":"", "mc_lesser":"1", "mc_price":"0.00", "mc_priority":"0", "mc_privs":"", "mc_arch":"x86_64"}'
JARVICE_MACHINES_ADD+=', {"mc_name":"n1", "mc_description":"4 core, 32GB RAM (CPU Only)", "mc_cores":"4", "mc_slots":"16", "mc_gpus":"0", "mc_ram":"32", "mc_swap":"16", "mc_scratch":"100", "mc_devices":"", "mc_properties":"node-role.kubernetes.io/jarvice-compute=", "mc_slave_properties":"", "mc_slave_gpus":"0", "mc_slave_ram":"32", "mc_scale_min":"1", "mc_scale_max":"1", "mc_scale_select":"", "mc_lesser":"1", "mc_price":"0.00", "mc_priority":"0", "mc_privs":"", "mc_arch":"x86_64"}'
JARVICE_MACHINES_ADD+=', {"mc_name":"n3", "mc_description":"16 core, 128GB RAM (CPU Only)", "mc_cores":"16", "mc_slots":"16", "mc_gpus":"0", "mc_ram":"128", "mc_swap":"64", "mc_scratch":"500", "mc_devices":"", "mc_properties":"node-role.kubernetes.io/jarvice-compute=", "mc_slave_properties":"", "mc_slave_gpus":"0", "mc_slave_ram":"128", "mc_scale_min":"1", "mc_scale_max":"256", "mc_scale_select":"", "mc_lesser":"1", "mc_price":"0.00", "mc_priority":"0", "mc_privs":"", "mc_arch":"x86_64"}'
# Add GPU machine types
JARVICE_MACHINES_ADD+=', {"mc_name":"ng0", "mc_description":"2 core, 16GB RAM with Nvidia GPU", "mc_cores":"2", "mc_slots":"16", "mc_gpus":"1", "mc_ram":"16", "mc_swap":"8", "mc_scratch":"64", "mc_devices":"", "mc_properties":"node-role.kubernetes.io/jarvice-compute=", "mc_slave_properties":"", "mc_slave_gpus":"0", "mc_slave_ram":"16", "mc_scale_min":"1", "mc_scale_max":"1", "mc_scale_select":"", "mc_lesser":"1", "mc_price":"0.00", "mc_priority":"0", "mc_privs":"", "mc_arch":"x86_64"}'
JARVICE_MACHINES_ADD+=', {"mc_name":"ng1", "mc_description":"4 core, 32GB RAM with Nvidia GPU", "mc_cores":"4", "mc_slots":"16", "mc_gpus":"1", "mc_ram":"32", "mc_swap":"16", "mc_scratch":"100", "mc_devices":"", "mc_properties":"node-role.kubernetes.io/jarvice-compute=", "mc_slave_properties":"", "mc_slave_gpus":"0", "mc_slave_ram":"32", "mc_scale_min":"1", "mc_scale_max":"1", "mc_scale_select":"", "mc_lesser":"1", "mc_price":"0.00", "mc_priority":"0", "mc_privs":"", "mc_arch":"x86_64"}'
JARVICE_MACHINES_ADD+=', {"mc_name":"ng3", "mc_description":"16 core, 128GB RAM with Nvidia GPU", "mc_cores":"16", "mc_slots":"16", "mc_gpus":"1", "mc_ram":"128", "mc_swap":"64", "mc_scratch":"500", "mc_devices":"", "mc_properties":"node-role.kubernetes.io/jarvice-compute=", "mc_slave_properties":"", "mc_slave_gpus":"1", "mc_slave_ram":"128", "mc_scale_min":"1", "mc_scale_max":"2", "mc_scale_select":"", "mc_lesser":"1", "mc_price":"0.00", "mc_priority":"0", "mc_privs":"", "mc_arch":"x86_64"}'
JARVICE_MACHINES_ADD+=']'

echo
echo "* Initializing helm and installing Tiller into cluster..."
$HELM init

echo
echo "* Creating/setting service account and cluster role binding for Tiller..."
tiller_sa_yaml=$(cat <<EOF
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller
    namespace: kube-system
EOF
)
echo "$tiller_sa_yaml" | $KUBECTL --namespace kube-system create -f -
$HELM init --upgrade --service-account tiller
$KUBECTL -n kube-system patch deployment tiller-deploy \
    -p '{"spec": {"template":{"spec":{"tolerations":[{"key": "node-role.kubernetes.io/jarvice-system", "effect": "NoSchedule", "operator": "Exists"}]}}}}'
sleep 1

echo
echo -n "* "
$KUBECTL --namespace kube-system rollout status \
    deployment tiller-deploy --watch=true

if [ -n "$install_dashboard" ]; then
    echo
    echo "* Installing kubernetes-dashboard..."
    $HELM install \
        --set tolerations[0].effect=NoSchedule \
        --set tolerations[0].key=node-role.kubernetes.io/jarvice-system \
        --set tolerations[0].operator=Exists \
        --namespace kube-system \
        --name kubernetes-dashboard \
        stable/kubernetes-dashboard
    $KUBECTL --namespace kube-system expose deployment kubernetes-dashboard \
        --type=LoadBalancer --name kubernetes-dashboard-lb
fi

# See: https://kubernetes.io/docs/concepts/storage/storage-classes/#gce-pd
classes="jarvice-db jarvice-user"
for class in $classes; do
    echo
    echo "* Creating '$class' StorageClass..."
    storage_class_yaml=$(cat <<EOF
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: $class
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-standard
  replication-type: none
  #replication-type: regional-pd
reclaimPolicy: Delete
mountOptions:
  - debug
EOF
)
    echo "$storage_class_yaml" | $KUBECTL --namespace kube-system create -f -
done

dbpv=$($GCLOUD compute disks list $gcloud_global_flags --format=json \
    --filter="zone:$cluster_zone AND name ~ ^gke-$cluster_name-.+-pvc- AND description ~ '\"kubernetes.io/created-for/pvc/name\":\"jarvice-db-pvc\"' " | \
    $JQ .[0])

vol_size=$(echo "$dbpv" | $JQ -r ".sizeGb")
pd_name=$(echo "$dbpv" | $JQ -r ".name")
if [ "$pd_name" != "null" ]; then

    # See: https://kubernetes.io/docs/concepts/storage/volumes/#gcepersistentdisk
    echo
    echo "* Creating 'jarvice-db-pv' PersistentVolume with pdName '$pd_name'..."
    pv_yaml=$(cat <<EOF
apiVersion: v1
kind: PersistentVolume
metadata:
  name: jarvice-db-pv
spec:
  gcePersistentDisk:
    fsType: ext4
    pdName: $pd_name
  capacity:
    storage: ${vol_size}Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
  storageClassName: jarvice-db
  mountOptions:
    - debug
EOF
)
    echo "$pv_yaml" | $KUBECTL --namespace kube-system create -f -
fi

jarvice_jobs_domain=$(get_yaml_value "jarvice.JARVICE_JOBS_DOMAIN")
jarvice_jobs_lb_service=$(get_yaml_value "jarvice.JARVICE_JOBS_LB_SERVICE")
jarvice_pvc_vault_size=$(get_yaml_value "jarvice.JARVICE_PVC_VAULT_SIZE")
jarvice_pvc_vault_name=$(get_yaml_value "jarvice.JARVICE_PVC_VAULT_NAME")
jarvice_pvc_vault_storageclass=$(get_yaml_value "jarvice.JARVICE_PVC_VAULT_STORAGECLASS")
jarvice_pvc_vault_accessmodes=$(get_yaml_value "jarvice.JARVICE_PVC_VAULT_ACCESSMODES")
echo
echo "* Running helm to deploy JARVICE into cluster..."
$HELM install --debug \
    --set jarvice.imagePullSecret.username="$registry_username" \
    --set jarvice.imagePullSecret.password="$registry_password" \
    --set jarvice.JARVICE_LICENSE_LIC="$jarvice_license" \
    --set jarvice.JARVICE_REMOTE_USER="$jarvice_username" \
    --set jarvice.JARVICE_REMOTE_APIKEY="$jarvice_apikey" \
    --set jarvice.JARVICE_JOBS_DOMAIN="$jarvice_jobs_domain" \
    --set jarvice.JARVICE_JOBS_LB_SERVICE="$jarvice_jobs_lb_service" \
    --set jarvice.JARVICE_PVC_VAULT_SIZE="$jarvice_pvc_vault_size" \
    --set jarvice.JARVICE_PVC_VAULT_NAME="$jarvice_pvc_vault_name" \
    --set jarvice.JARVICE_PVC_VAULT_STORAGECLASS="$jarvice_pvc_vault_storageclass" \
    --set jarvice.JARVICE_PVC_VAULT_ACCESSMODES="$jarvice_pvc_vault_accessmodes" \
    --set jarvice.daemonsets.lxcfs.enabled="true" \
    --set jarvice.daemonsets.nvidia.enabled="false" \
    --set jarvice_db.persistence.enabled="true" \
    --set jarvice_dal.env.JARVICE_MACHINES_ADD="$(echo "$JARVICE_MACHINES_ADD" | sed -e 's#\([]{}",[/]\)#\\\1#g')" \
    --name $helm_name --namespace $helm_namespace $jarvice_chart_dir
    #--set jarvice_db.persistence.storageClass="-" \
    #--set jarvice_db.persistence.volumeName="jarvice-db-$cluster_name" \
sleep 1

echo
echo "* Installing Traefik ingress controller..."
cert_file=$(get_yaml_value "jarvice.ingress.cert_file")
key_file=$(get_yaml_value "jarvice.ingress.key_file")
traefik_lb_ip=$(get_yaml_value "jarvice.ingress.loadBalancerIP")

[ -n "$cert_file" ] && default_cert="$(cat $cert_file | base64 -w 0)" && \
    default_cert="--set ssl.defaultCert=$default_cert"
[ -n "$key_file" ] && default_key="$(cat $key_file | base64 -w 0)" && \
    default_key="--set ssl.defaultKey=$default_key"
[ -n "$traefik_lb_ip" ] && lb_ip="--set loadBalancerIP=$traefik_lb_ip"

    #--set nodeSelector."node-role\.kubernetes\.io/jarvice-system"="" \
$HELM install --debug \
    --set tolerations[0]."key"="node-role\.kubernetes\.io/jarvice-system" \
    --set tolerations[0]."effect"="NoSchedule" \
    --set tolerations[0]."operator"="Exists" \
    --set rbac.enabled=true \
    --set ssl.enabled=true \
    --set ssl.enforced=true \
    --set ssl.permanentRedirect=true \
    --set ssl.insecureSkipVerify=true \
    --set replicas=3 \
    --set memoryRequest=1Gi --set memoryLimit=1Gi \
    --set cpuRequest=1 --set cpuLimit=1 \
    $default_cert $default_key $lb_ip \
    --namespace kube-system --name traefik stable/traefik
#    --set dashboard.enabled=true \
#    --set dashboard.domain=traefik-dashboard.<domain> \

echo
echo "* Searching for user disks previously created for '$cluster_name' in zone '$cluster_zone'..."
user_disks=$($GCLOUD compute disks list $gcloud_global_flags --format=json \
    --filter="zone:$cluster_zone AND name ~ ^gke-$cluster_name-.+-pvc- AND description ~ '\"kubernetes.io/created-for/pvc/namespace\":\"jarvice-system-jobs\"' " | \
    $JQ -r .[].name)
    

for disk in $user_disks; do
    disk_json=$($GCLOUD compute disks describe $gcloud_global_flags \
        --format=json --zone=$cluster_zone $disk)
    vol_size=$(echo "$disk_json" | $JQ -r ".sizeGb")
    pd_name=$(echo "$disk_json" | $JQ -r ".name")
    pv_name=$(echo "$disk_json" | $JQ -r ".description" | $JQ -r '.["kubernetes.io\/created-for\/pv\/name"]')
    pvc_name=$(echo "$disk_json" | $JQ -r ".description" | $JQ -r '.["kubernetes.io\/created-for\/pvc\/name"]')
    echo "Found ${vol_size}GB disk '$pd_name' in zone '$cluster_zone'..."
    echo
    echo "* Recreating PersistentVolume '$pv_name' using '$pd_name' for cluster '$cluster_name'..."
    pv_yaml=$(cat <<EOF
apiVersion: v1
kind: PersistentVolume
metadata:
  name: $pv_name
spec:
  gcePersistentDisk:
    fsType: ext4
    pdName: $pd_name
  capacity:
    storage: ${vol_size}Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
  storageClassName: jarvice-user
  mountOptions:
    - debug
EOF
)
    echo "$pv_yaml" | $KUBECTL --namespace kube-system create -f -

    echo
    echo "* Recreating PersistentVolumeClaim '$pvc_name' in '$helm_namespace-jobs' namespace..."
    pvc_yaml=$(cat <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: $pvc_name
spec:
  resources:
    requests:
      storage: ${vol_size}Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  storageClassName: jarvice-user
  volumeName: $pv_name
EOF
)
    echo "$pvc_yaml" | $KUBECTL --namespace $helm_namespace-jobs create -f -
done

if [ -z "$user_disks" ]; then
    echo "No previous user disks were found."
fi

echo
echo "* Waiting for rollout of JARVICE components..."
DEPLOYMENTS="jarvice-dal jarvice-scheduler jarvice-api"
DEPLOYMENTS+=" jarvice-mc-portal jarvice-appsync"
for deployment in $DEPLOYMENTS; do
    echo
    echo -n "* "
    $KUBECTL --namespace $helm_namespace rollout status \
        deployment $deployment --watch=true
done

echo
echo "------------------------------------------------------------------------"

echo
echo "JARVICE is deployed and ready!"
echo
echo "This cluster may be deleted by executing:"
echo "\$ $0 --cluster-delete --config-file $config_file"

if [ -n "$OLD_CONTEXT" ]; then
    echo
    echo "kubeconfig file ($KUBECONFIG) has been modified."
    echo "Revert to the previous kubeconfig current-context by executing:"
    echo "\$ kubectl config set current-context $OLD_CONTEXT"
fi

API_HOST=$($KUBECTL --namespace $helm_namespace \
    describe services jarvice-api-lb | \
    grep Ingress | awk '{print $3}')
PORTAL_HOST=$($KUBECTL --namespace $helm_namespace \
    describe services jarvice-mc-portal-lb | \
    grep Ingress | awk '{print $3}')

if [ -n "$install_dashboard" ]; then
    DASHBOARD_HOST=$($KUBECTL --namespace $helm_namespace \
        describe services kubernetes-dashboard-lb | \
        grep Ingress | awk '{print $3}')
    echo
    echo "Kubernetes dashbaord is available at:"
    echo "https://$DASHBOARD_HOST:8443/"
fi
TRAEFIK_HOST=$($KUBECTL --namespace kube-system \
    describe services traefik | \
    grep Ingress | awk '{print $3}')


echo
echo "JARVICE API endpoint is available at:"
echo "https://$API_HOST/"
echo
echo "JARVICE portal is available at:"
echo "https://$PORTAL_HOST/"
echo
echo "Optionally, set up DNS for Traefik ingress available at:"
echo "$TRAEFIK_HOST"
echo

