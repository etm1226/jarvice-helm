#!/bin/bash

#set -x

if [ "$(arch)" != "x86_64" ]; then
    echo "This script only supports x86_64 architecture at the moment."
    exit 1
fi

eks_cluster_name=jarvice
eks_cluster_delete=
eks_node_type=c5.9xlarge
install_nvidia_plugin=
eks_nodes=4
eks_nodes_max=0
eks_nodes_vol_size=100
aws_region=us-west-2
aws_zones=
helm_name=jarvice
helm_namespace=jarvice-system
eks_stack_add=
eks_stack_update=
eks_stack_delete=

function jarvice_deploy2eks_usage {
    cat <<EOF
Usage:
  $0 [deploy_options] [eks_cluster_options]
  $0 --eks-stack-add [eks_cluster_options]
  $0 --eks-stack-update <number> [eks_cluster_options]
  $0 --eks-stack-delete <number> [--eks-cluster-name <name>] [--aws-region <aws_region>]
  $0 --eks-stack-get <number> [--eks-cluster-name <name>] [--aws-region <aws_region>]
  $0 --eks-cluster-delete <name> [--aws-region <aws_region>]

Available [deploy_options]:
  --registry-username <username>    Docker registry username for JARVICE system
                                    images
  --registry-password <password>    Docker registry password for JARVICE system
                                    images
  --jarvice-license <license_key>   JARVICE license key
  --jarvice-username <username>     JARVICE platform username for app
                                    synchronization
  --jarvice-apikey <apikey>         JARVICE platform apikey for app
                                    synchronization
  --jarvice-chart-dir <path>        Alternative JARVICE helm chart directory
                                    (optional)
  --helm-name <app_release_name>    Helm app release name
                                    (default: $helm_name)
  --helm-namespace <k8s_namespace>  Cluster namepace to install release into
                                    (default: $helm_namespace)

Available [eks_cluster_options]:
  --aws-region <aws_region>         AWS region for EKS cluster
                                    (default: $aws_region)
  --aws-zones <aws_zone_list>       Comma separated zone list for --aws-region
                                    (initial deploy only, optional)
  --eks-cluster-name <name>         EKS cluster name
                                    (default: $eks_cluster_name)
  --eks-node-type <node_type>       EC2 instance types for EKS nodes
                                    (default: $eks_node_type)
  --eks-nodes <number>              Number of EKS cluster nodes
                                    (default: $eks_nodes)
  --eks-nodes-max <number>          Autoscale up to maximum number of nodes
                                    (must be greater than --eks-nodes)
  --eks-nodes-vol-size <number>     Size of the nodes' EBS volume in GB
                                    (default: $eks_nodes_vol_size)
  --install-nvidia-plugin           Install kubernetes device plugin if
                                    --eks-node-type has Nvidia GPUs
                                    (initial deploy only, optional)

See the following link for available EC2 instance types (--eks-node-type):
https://aws.amazon.com/ec2/instance-types/

Example (minimal) deploy command:
\$ $0 \\
    --registry-username <username> \\
    --registry-password <password> \\
    --jarvice-license <license_key> \\
    --jarvice-username <username> \\
    --jarvice-apikey <apikey>

Example (minimal) delete command (must explicitly supply cluster name):
\$ $0 --eks-cluster-delete <name>

EOF
}

while [ $# -gt 0 ]; do
    case $1 in
        --help)
            jarvice_deploy2eks_usage
            exit 0
            ;;
        --registry-username)
            registry_username=$2
            shift; shift
            ;;
        --registry-password)
            registry_password=$2
            shift; shift
            ;;
        --jarvice-license)
            jarvice_license=$2
            shift; shift
            ;;
        --jarvice-username)
            jarvice_username=$2
            shift; shift
            ;;
        --jarvice-apikey)
            jarvice_apikey=$2
            shift; shift
            ;;
        --jarvice-chart-dir)
            jarvice_chart_dir=$2
            shift; shift
            ;;
        --eks-cluster-name)
            eks_cluster_name=$2
            shift; shift
            ;;
        --eks-cluster-delete)
            eks_cluster_delete=$2
            shift; shift
            ;;
        --eks-node-type)
            eks_node_type=$2
            shift; shift
            ;;
        --install-nvidia-plugin)
            install_nvidia_plugin=y
            shift
            ;;
        --eks-nodes)
            eks_nodes=$2
            shift; shift
            ;;
        --eks-nodes-max)
            eks_nodes_max=$2
            shift; shift
            ;;
        --eks-nodes-vol-size)
            eks_nodes_vol_size=$2
            shift; shift
            ;;
        --aws-region)
            aws_region=$2
            shift; shift
            ;;
        --aws-zones)
            aws_zones=$2
            shift; shift
            ;;
        --helm-name)
            helm_name=$2
            shift; shift
            ;;
        --helm-namespace)
            helm_namespace=$2
            shift; shift
            ;;
        --eks-stacks-get)
            eks_stacks_get=y
            shift
            ;;
        --eks-stack-add)
            eks_stack_add=y
            shift
            ;;
        --eks-stack-update)
            eks_stack_update=$2
            shift; shift
            ;;
        --eks-stack-delete)
            eks_stack_delete=$2
            shift; shift
            ;;
        *)
            jarvice_deploy2eks_usage
            exit 1
            ;;
    esac
done

KUBECTL=$(type -p kubectl)
AWS=$(type -p aws)
AWS_IAM_AUTH=$(type -p aws-iam-authenticator)
EKSCTL=$(type -p eksctl)
HELM=$(type -p helm)
CURL=$(type -p curl)
UNZIP=$(type -p unzip)
JQ=$(type -p jq)

KUBECTL_VER_REQ=1.10
EKSCTL_VER_REQ=0.1.2

if [ -z "$KUBECONFIG" ]; then
    KUBECONFIG=~/.kube/config
fi
export KUBECONFIG

function check_deps {
    if [ -z "$KUBECTL" -o -z "$AWS" -o -z "$AWS_IAM_AUTH" -o -z "$EKSCTL" -o -z "$HELM" -o -z "$JQ" ]; then
        echo "Missing software needs to be installed.  Verifying sudo access..."
        SUID=$(sudo id -u)
        if [ "$SUID" != "0" ]; then
            echo "Could not verify sudo access.  Cannot continue."
            echo "Please resolve sudo access before re-running this script."
            exit 1
        else
            echo "Verified sudo access..."
        fi
        install_pkgs=
        if [ -z "$CURL" ]; then
            echo "curl command not found...  Installing..."
            install_pkgs+="curl "
        fi
        if [ -z "$JQ" ]; then
            echo "jq command not found...  Installing..."
            install_pkgs+="jq "
        fi
        if [ -n "$install_pkgs" ]; then
            if [ -e /etc/redhat-release ]; then
                sudo yum -y install $install_pkgs
            else
                sudo apt-get -y update
                sudo apt-get -y install $install_pkgs
            fi
        fi
    fi
}

function get_json_value {
    json=$1
    key=$2
    echo "$json" | \
        python -c "import json,sys;obj=json.load(sys.stdin);print obj$key;" \
        2>/dev/null || /bin/true
}

function strict_version {
    ver=$1
    ver_req=$2
    python -c "from distutils.version import StrictVersion;print StrictVersion('$ver') >= StrictVersion('$ver_req');"
}

function eks_cf_stack_delete {
    stack_name="$1"

    echo
    echo "* Deleting CloudFormation stack '$stack_name'..."
    $AWS --region $aws_region cloudformation \
        delete-stack --stack-name $stack_name
    status="DELETE_IN_PROGRESS"
    while [ "$status" = "DELETE_IN_PROGRESS" ]; do
        echo -n "."
        sleep 5
        status=$($AWS --region $aws_region cloudformation \
            describe-stacks --stack-name $stack_name --output=json \
            2>/dev/null || /bin/true)
        status=$(get_json_value "$status" "['Stacks'][0]['StackStatus']")
        status_reason=$(get_json_value "$status" "['Stacks'][0]['StackStatusReason']")
    done
    echo "$status"
    if [ -n "$status_reason" ]; then
        echo "$status_reason"
    fi
}

function eks_nodegroup_stacks_get {
    nodegroup_ids=$($AWS --region $aws_region cloudformation \
        describe-stacks | \
        jq -r ".Stacks[] | select(.Parameters[].ParameterValue==\"eksctl-$eks_cluster_name-cluster\").Parameters[] | select(.ParameterKey==\"NodeGroupID\").ParameterValue" | \
            sort)
    echo -e "ID\tnode_type\tmin_nodes\tmax_nodes\tvol_size"
    echo -e "--\t---------\t---------\t---------\t--------"
    for nodegroup_id in $nodegroup_ids; do
        stack_name="eksctl-$eks_cluster_name-nodegroup-$nodegroup_id"
        template_body=$($AWS --region $aws_region cloudformation \
            get-template \
            --stack-name $stack_name 2>/dev/null | \
            jq ".TemplateBody")
        echo "$template_body" >/tmp/tbody.json
        node_type=$(echo "$template_body" | \
            jq -r ".Resources.NodeLaunchConfig.Properties.InstanceType")
        min_size=$(echo "$template_body" | \
            jq -r ".Resources.NodeGroup.Properties.MinSize")
        max_size=$(echo "$template_body" | \
            jq -r ".Resources.NodeGroup.Properties.MaxSize")
        vol_size=$(echo "$template_body" | \
            jq -r ".Resources.NodeLaunchConfig.Properties.BlockDeviceMappings[0].Ebs.VolumeSize")
        echo -e "$nodegroup_id\t$node_type\t\t$min_size\t\t$max_size\t$vol_size GB"
    done
}

function eks_nodegroup_stack_add_update {
    nodegroup_id=$1

    stack_name=
    # Get template body of previous node stack if updating
    if [ -n "$nodegroup_id" ]; then
        command=update-stack
        stack_name="eksctl-$eks_cluster_name-nodegroup-$nodegroup_id"
        template_body=$($AWS --region $aws_region cloudformation \
            get-template \
            --stack-name $stack_name 2>/dev/null | \
            jq ".TemplateBody")
    else
        command=create-stack
        template_body=$($AWS --region $aws_region cloudformation \
            get-template \
            --stack-name eksctl-$eks_cluster_name-nodegroup-0 | \
            jq ".TemplateBody")
        nodegroup_id=$($AWS --region $aws_region cloudformation \
            describe-stacks | \
            jq -r ".Stacks[] | select(.Parameters[].ParameterValue==\"eksctl-$eks_cluster_name-cluster\").Parameters[] | select(.ParameterKey==\"NodeGroupID\").ParameterValue" | \
            sort | tail -n 1)
        nodegroup_id=$((nodegroup_id+1))
        stack_name="eksctl-$eks_cluster_name-nodegroup-$nodegroup_id"
    fi

    map=$(cat <<EOF
{
    "DeviceName": "/dev/xvda",
    "Ebs": {
        "VolumeSize": "$eks_nodes_vol_size",
        "VolumeType": "gp2",
        "DeleteOnTermination": true,
    }
}
EOF
)
    [ $eks_nodes_max -eq 0 ] && eks_nodes_max=$eks_nodes
    template_body=$(echo "$template_body" | \
        jq ".Resources.NodeLaunchConfig.Properties.BlockDeviceMappings = [$map]" | \
        jq ".Resources.NodeLaunchConfig.Properties.InstanceType = \"$eks_node_type\"" | \
        jq ".Resources.NodeGroup.Properties.DesiredCapacity = \"$eks_nodes\"" | \
        jq ".Resources.NodeGroup.Properties.MinSize = \"$eks_nodes\"" | \
        jq ".Resources.NodeGroup.Properties.MaxSize = \"$eks_nodes_max\"")

    if [ "$command" = "update-stack" ]; then
        echo
        echo "* Updating CloudFormation stack '$stack_name'..."
    else
        echo
        echo "* Adding CloudFormation stack '$stack_name'..."
    fi

    $AWS --region $aws_region cloudformation \
        $command \
        --stack-name $stack_name \
        --capabilities CAPABILITY_IAM \
        --template-body "$template_body" \
        --tags \
            Key=eksctl.cluster.k8s.io/v1alpha1/cluster-name,Value=$eks_cluster_name \
            Key=eksctl.cluster.k8s.io/v1alpha1/nodegroup-id,Value=$nodegroup_id \
        --parameters \
            ParameterKey=ClusterName,ParameterValue=$eks_cluster_name \
            ParameterKey=ClusterStackName,ParameterValue=eksctl-$eks_cluster_name-cluster \
            ParameterKey=NodeGroupID,ParameterValue=$nodegroup_id >/dev/null

    base_status="CREATE_IN_PROGRESS"
    [ "$command" = "update-stack" ] && base_status="UPDATE_IN_PROGRESS"
    status=$base_status
    while [ "$status" = "$base_status" ]; do
        echo -n "."
        sleep 5
        status=$($AWS --region $aws_region cloudformation \
            describe-stacks \
            --stack-name $stack_name --output=json \
            2>/dev/null || /bin/true)
        status=$(get_json_value "$status" "['Stacks'][0]['StackStatus']")
        status_reason=$(get_json_value "$status" "['Stacks'][0]['StackStatusReason']")
    done
    echo "$status"
    if [ -n "$status_reason" ]; then
        echo "$status_reason"
        return
    fi

    echo
    echo "* Updating aws-auth ConfigMap..."
    role_arns=$($AWS --region $aws_region cloudformation \
        describe-stacks | \
        jq -r ".Stacks[] | select(.Parameters[].ParameterValue==\"eksctl-$eks_cluster_name-cluster\").Outputs[].OutputValue")
        configmap_yaml=$(cat <<EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: aws-auth
  namespace: kube-system
data:
  mapRoles: |
EOF
)
    for role_arn in $role_arns; do
        configmap_yaml=$(cat <<EOF
$configmap_yaml
    - rolearn: $role_arn
      username: system:node:{{EC2PrivateDNSName}}
      groups:
        - system:bootstrappers
        - system:nodes
EOF
)
    done
    echo "$configmap_yaml" | $KUBECTL --namespace kube-system apply -f - >/dev/null 2>&1
}

if [ -n "$eks_cluster_delete" ]; then
    set -e
    check_deps

    echo
    echo "* Using $KUBECONFIG for kubeconfig..."

    echo
    echo "* Deleting EKS cluster '$eks_cluster_delete' in region" \
        "'$aws_region'..."

    # eksctl doesn't delete EC2 load balancers and ELB security groups
    # associated with it.  So we're doing it the hard way..
    TAG_KEY_VAL="kubernetes.io/cluster/$eks_cluster_delete"
    AWS_ELB_SEC_GROUP_NAMES=$($AWS --region $aws_region ec2 \
        describe-security-groups --no-paginate --output=text \
        --filters Name=tag-key,Values=$TAG_KEY_VAL \
        --query "SecurityGroups[*].{Name:GroupName}" | grep '^k8s-elb-' \
        || /bin/true)
    AWS_ELB_NAMES=$(echo "$AWS_ELB_SEC_GROUP_NAMES" | sed -e 's/k8s-elb-//')
    echo
    for elb_name in $AWS_ELB_NAMES; do
        echo "* Deleting AWS elastic load balancer '$elb_name'..."
        $AWS --region $aws_region elb delete-load-balancer \
            --load-balancer-name=$elb_name
    done

    nodegroup_stacks=$($AWS --region $aws_region cloudformation \
        describe-stacks | \
        jq -r ".Stacks[] | select(.Parameters[].ParameterValue==\"eksctl-$eks_cluster_delete-cluster\").StackName")
    for nodegroup_stack in $nodegroup_stacks; do
        eks_cf_stack_delete "$nodegroup_stack"
    done

    AWS_ELB_SEC_GROUP_IDS=$($AWS --region $aws_region ec2 \
        describe-security-groups --no-paginate --output=text \
        --filters Name=tag-key,Values=$TAG_KEY_VAL \
        --query "SecurityGroups[*].{ID:GroupId}")
        #--filters Name=tag:eksctl.cluster.k8s.io/v1alpha1/cluster-name,Values=$eks_cluster_delete \

    echo
    for group_id in $AWS_ELB_SEC_GROUP_IDS; do
        echo "* Deleting AWS EC2 security group '$group_id'..."
        $AWS --region $aws_region ec2 delete-security-group \
            --group-id=$group_id
    done

    eks_cf_stack_delete "eksctl-$eks_cluster_delete-cluster"

    #echo "* Deleting EKS cluster '$eks_cluster_delete' in region" \
    #    "'$aws_region'..."
    #$EKSCTL delete cluster --name $eks_cluster_delete --region $aws_region
    #echo
    #echo "EKS cluster '$eks_cluster_delete' is in the process of being" \
    #    "deleted from '$aws_region'..."
    #echo

    exit 0
fi

if [ $eks_nodes_max -gt 0 -a $eks_nodes_max -le $eks_nodes ]; then
    echo "--eks-nodes-max must be greater than --eks-nodes in order to" \
        "enable node auto scaling."
    exit 1
fi

if [ -n "$eks_stacks_get" ]; then
    set -e
    check_deps

    if [ -z "$eks_cluster_name" ]; then
        jarvice_deploy2eks_usage
    fi
    eks_nodegroup_stacks_get

    exit 0
fi

if [ -n "$eks_stack_add" ]; then
    set -e
    check_deps

    if [ -z "$eks_cluster_name" ]; then
        jarvice_deploy2eks_usage
    fi
    eks_nodegroup_stack_add_update

    exit 0
fi

if [ -n "$eks_stack_update" ]; then
    set -e
    check_deps

    if [ -z "$eks_cluster_name" ]; then
        jarvice_deploy2eks_usage
    fi
    eks_nodegroup_stack_add_update $eks_stack_update

    exit 0
fi

if [ -n "$eks_stack_delete" ]; then
    set -e
    check_deps

    if [ "$eks_stack_delete" = "0" ]; then
        echo "Nodegroup 0 may only be updated, not deleted."
        exit 1
    fi

    if [ -z "$eks_cluster_name" ]; then
        jarvice_deploy2eks_usage
    fi
    stack_name="eksctl-$eks_cluster_name-nodegroup-$eks_stack_delete"
    eks_cf_stack_delete "$stack_name"

    exit 0
fi

if [ -z "$registry_username" -o -z "$registry_password" -o -z "$jarvice_license" -o -z "$jarvice_username" -o -z "$jarvice_apikey" ]; then
    echo "Missing credentials and/or license.  Cannot continue."
    jarvice_deploy2eks_usage
fi

# Assumes helm chart basedir is one level up from this script
[ -z "$jarvice_chart_dir" ] && jarvice_chart_dir=$(dirname $(dirname $0))
echo "Verifying helm chart directory '$jarvice_chart_dir'..."
CHART_NAME=$(grep '^name: jarvice' $jarvice_chart_dir/Chart.yaml 2>/dev/null)
if [ -z "$CHART_NAME" ]; then
    echo "Could not verify helm chart for JARVICE.  Cannot continue."
    echo "Use --jarvice-chart-dir to specify valid JARVICE helm chart" \
        "directory."
    exit 1
else
    echo "Found valid chart directory..."
fi

set -e
check_deps

[ -z "$INSTALL_DIR" ] && INSTALL_DIR=/usr/local

if [ -z "$KUBECTL" ]; then
    KUBECTL=$INSTALL_DIR/bin/kubectl
    echo "kubectl command not found...  Installing to $KUBECTL..."
    sudo bash -c "curl --silent --location https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl >$KUBECTL"
    sudo chown root.root $KUBECTL
    sudo chmod 755 $KUBECTL
else
    KUBECTL_MAJOR=$($KUBECTL version | grep '^Client' | sed -E 's/.*Major:"([0-9]+)".*/\1/')
    KUBECTL_MINOR=$($KUBECTL version | grep '^Client' | sed -E 's/.*Minor:"([0-9]+)\+*".*/\1/')
    KUBECTL_VER="$KUBECTL_MAJOR.$KUBECTL_MINOR"
    echo "Found $KUBECTL... Version: $KUBECTL_VER"
    if [ "$(strict_version $KUBECTL_VER $KUBECTL_VER_REQ)" != "True" ]; then
        echo "kubectl version $KUBECTL_VER_REQ or newer is required."
        echo "Please upgrade kubectl or remove it."
        echo "This script will re-install kubectl if it is removed."
        exit 1
    fi
fi

if [ -z "$AWS" ]; then
    AWS=$INSTALL_DIR/bin/aws
    echo "aws command not found...  Installing to $AWS..."
    if [ -z "$UNZIP" ]; then
        echo "unzip command not found...  Installing..."
        if [ -e /etc/redhat-release ]; then
            sudo yum -y install unzip
        else
            sudo apt-get -y update
            sudo apt-get -y install unzip
        fi
    fi
    curl --silent --location "https://s3.amazonaws.com/aws-cli/awscli-bundle.zip" >/tmp/awscli-bundle.zip
    unzip -d /tmp /tmp/awscli-bundle.zip >/dev/null
    sudo /tmp/awscli-bundle/install -i $INSTALL_DIR/aws -b $AWS
    rm -rf /tmp/awscli-bundle*
else
    echo "Found $AWS..."
fi

AWS_IAM_USER=$($AWS --output=text iam get-user | awk '{print $NF}')
if [ -z "$AWS_IAM_USER" ]; then
    echo "AWS IAM user not found..."
    echo "Please set/export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables: https://docs.aws.amazon.com/cli/latest/userguide/cli-environment.html"
    echo "Or set aws_access_key_id and aws_secret_access_key in your credentials file: https://docs.aws.amazon.com/cli/latest/userguide/cli-config-files.html"
    exit 1
else
    echo "Found AWS IAM user... User name: $AWS_IAM_USER"
fi

if [ -z "$AWS_IAM_AUTH" ]; then
    AWS_IAM_AUTH=$INSTALL_DIR/bin/aws-iam-authenticator
    echo "aws-iam-authenticator command not found...  Installing to $AWS_IAM_AUTH..."
    sudo bash -c "curl --silent --location https://amazon-eks.s3-us-west-2.amazonaws.com/1.10.3/2018-07-26/bin/linux/amd64/aws-iam-authenticator >$AWS_IAM_AUTH"
    sudo chown root.root $AWS_IAM_AUTH
    sudo chmod 755 $AWS_IAM_AUTH
else
    echo "Found $AWS_IAM_AUTH..."
fi

if [ -z "$EKSCTL" ]; then
    EKSCTL=$INSTALL_DIR/bin/eksctl
    echo "eksctl command not found...  Installing to $EKSCTL..."
    curl --silent --location "https://github.com/weaveworks/eksctl/releases/download/latest_release/eksctl_$(uname -s)_amd64.tar.gz" | sudo tar xz -C /usr/local/bin
    sudo chown root.root $EKSCTL
    sudo chmod 755 $EKSCTL
else
    EKSCTL_VER=$($EKSCTL version | sed -e 's/.*{/{/')
    EKSCTL_VER=$(echo "$EKSCTL_VER" | jq -r '.gitTag')
    echo "Found $EKSCTL... Version: $EKSCTL_VER"

    if [ "$(strict_version $EKSCTL_VER $EKSCTL_VER_REQ)" != "True" ]; then
        echo "eksctl version $EKSCTL_VER_REQ or newer is required."
        echo "Please upgrade eksctl or remove it."
        echo "This script will re-install eksctl if it is removed."
        exit 1
    fi
fi

if [ -z "$HELM" ]; then
    HELM_INSTALL_DIR=$INSTALL_DIR/bin
    HELM=$HELM_INSTALL_DIR/helm
    echo "helm command not found...  Installing to $HELM..."
    curl --silent https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get >/tmp/get_helm.sh
    sudo --preserve-env=PATH,HELM_INSTALL_DIR bash /tmp/get_helm.sh >/dev/null
    rm -f /tmp/get_helm.sh
else
    echo "Found $HELM..."
fi

echo "------------------------------------------------------------------------"

echo
echo "* Using $KUBECONFIG for kubeconfig..."
OLD_CONTEXT=$($KUBECTL config current-context 2>/dev/null || /bin/true)

echo
echo "* Running $EKSCTL to create EKS cluster..."
#num_nodes="--nodes $eks_nodes"
#[ $eks_nodes_max -gt $eks_nodes ] && \
#    num_nodes="--nodes-min $eks_nodes --nodes-max $eks_nodes_max"
#$EKSCTL create cluster --name $eks_cluster_name \
#    --region $aws_region ${aws_zones:+--zones=$aws_zones} \
#    --node-type $eks_node_type $num_nodes
$EKSCTL create cluster --name $eks_cluster_name \
    --region $aws_region ${aws_zones:+--zones=$aws_zones} \
    --node-type $eks_node_type --nodes 0

echo
echo "* Updating nodegroup stack..."
eks_nodegroup_stack_add_update 0

echo
echo "* Waiting for minimum number of nodes to be ready..."
ready_nodes=0
echo "$ready_nodes of $eks_nodes are ready..."
while [ $ready_nodes -lt $eks_nodes ]; do
    sleep 5
    echo -n "."
    ready_count=$(kubectl get nodes 2>/dev/null | \
        grep --count Ready || /bin/true)
    if [ $ready_count -gt $ready_nodes ]; then
        ready_nodes=$ready_count
        echo; echo "$ready_nodes of $eks_nodes are ready..."
    fi
done

if [ -n "$install_weave_plugin" ]; then
    echo
    echo "* Installing Weave Net network plugin into cluster..."
    $KUBECTL apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
fi

JARVICE_MACHINES_ADD='[{"mc_name":"n0", "mc_description":"2 core, 16GB RAM (CPU only)", "mc_cores":"2", "mc_slots":"16", "mc_gpus":"0", "mc_ram":"16", "mc_swap":"8", "mc_scratch":"64", "mc_devices":"", "mc_properties":"", "mc_slave_properties":"", "mc_slave_gpus":"0", "mc_slave_ram":"16", "mc_scale_min":"1", "mc_scale_max":"1", "mc_scale_select":"", "mc_lesser":"1", "mc_price":"0.00", "mc_priority":"0", "mc_privs":"", "mc_arch":"x86_64"}, {"mc_name":"n1", "mc_description":"4 core, 32GB RAM (CPU Only)", "mc_cores":"4", "mc_slots":"16", "mc_gpus":"0", "mc_ram":"32", "mc_swap":"16", "mc_scratch":"100", "mc_devices":"", "mc_properties":"", "mc_slave_properties":"", "mc_slave_gpus":"0", "mc_slave_ram":"32", "mc_scale_min":"1", "mc_scale_max":"1", "mc_scale_select":"", "mc_lesser":"1", "mc_price":"0.00", "mc_priority":"0", "mc_privs":"", "mc_arch":"x86_64"}'
if [ -n "$install_nvidia_plugin" ]; then
    echo
    echo "* Installing Nvidia device plugin into cluster..."
    SERVER_MAJOR=$($KUBECTL version | grep '^Server' | sed -E 's/.*Major:"([0-9]+)".*/\1/')
    SERVER_MINOR=$($KUBECTL version | grep '^Server' | sed -E 's/.*Minor:"([0-9]+\+*)".*/\1/')
    $KUBECTL create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v$SERVER_MAJOR.$SERVER_MINOR/nvidia-device-plugin.yml
    JARVICE_MACHINES_ADD+=', {"mc_name":"ng0", "mc_description":"2 core, 16GB RAM with Nvidia GPU", "mc_cores":"2", "mc_slots":"16", "mc_gpus":"1", "mc_ram":"16", "mc_swap":"8", "mc_scratch":"64", "mc_devices":"", "mc_properties":"", "mc_slave_properties":"", "mc_slave_gpus":"0", "mc_slave_ram":"16", "mc_scale_min":"1", "mc_scale_max":"1", "mc_scale_select":"", "mc_lesser":"1", "mc_price":"0.00", "mc_priority":"0", "mc_privs":"", "mc_arch":"x86_64"}, {"mc_name":"ng1", "mc_description":"4 core, 32GB RAM with Nvidia GPU", "mc_cores":"4", "mc_slots":"16", "mc_gpus":"1", "mc_ram":"32", "mc_swap":"16", "mc_scratch":"100", "mc_devices":"", "mc_properties":"", "mc_slave_properties":"", "mc_slave_gpus":"0", "mc_slave_ram":"32", "mc_scale_min":"1", "mc_scale_max":"1", "mc_scale_select":"", "mc_lesser":"1", "mc_price":"0.00", "mc_priority":"0", "mc_privs":"", "mc_arch":"x86_64"}'
fi
JARVICE_MACHINES_ADD+=']'

echo
echo "* Initializing helm and installing Tiller into cluster..."
$HELM init

echo
echo "* Creating/setting service account and cluster role binding for Tiller..."
tiller_sa_yaml=$(cat <<EOF
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller
    namespace: kube-system
EOF
)
echo "$tiller_sa_yaml" | $KUBECTL --namespace kube-system create -f -
$HELM init --upgrade --service-account tiller
sleep 1

echo
echo -n "* "
$KUBECTL --namespace kube-system rollout status \
    deployment tiller-deploy --watch=true

# TODO: revert replicaCount once we sort out the load balancer issue
echo
echo "* Running helm to deploy JARVICE into cluster..."
$HELM install --debug \
    --set jarvice.imagePullSecret.username="$registry_username" \
    --set jarvice.imagePullSecret.password="$registry_password" \
    --set jarvice.JARVICE_LICENSE_LIC="$jarvice_license" \
    --set jarvice.JARVICE_REMOTE_USER="$jarvice_username" \
    --set jarvice.JARVICE_REMOTE_APIKEY="$jarvice_apikey" \
    --set jarvice_mc_portal.replicaCount=1 \
    --set jarvice_dal.replicaCount=1 \
    --set jarvice_dal.env.JARVICE_MACHINES_ADD="$(echo "$JARVICE_MACHINES_ADD" | sed -e 's#\([]{}",[/]\)#\\\1#g')" \
    --name $helm_name --namespace $helm_namespace $jarvice_chart_dir
sleep 1

DEPLOYMENTS="jarvice-dal jarvice-scheduler jarvice-api"
DEPLOYMENTS+=" jarvice-mc-portal jarvice-appsync"
for deployment in $DEPLOYMENTS; do
    echo
    echo -n "* "
    $KUBECTL --namespace $helm_namespace rollout status \
        deployment $deployment --watch=true
done

echo
echo "------------------------------------------------------------------------"

echo
echo "JARVICE is deployed and ready!"
echo
echo "This EKS cluster may be deleted by executing:"
echo "\$ $0 --eks-cluster-delete $eks_cluster_name --aws-region $aws_region"

if [ -n "$OLD_CONTEXT" ]; then
    echo
    echo "kubeconfig file ($KUBECONFIG) has been modified."
    echo "Revert to the previous kubeconfig current-context by executing:"
    echo "\$ kubectl config set current-context $OLD_CONTEXT"
fi

API_HOST=$($KUBECTL --namespace $helm_namespace \
    get services jarvice-api-lb \
    -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
PORTAL_HOST=$($KUBECTL --namespace $helm_namespace \
    get services jarvice-mc-portal-lb \
    -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
[ -z "$API_HOST" ] && API_HOST=$($KUBECTL --namespace $helm_namespace \
    get services jarvice-api-lb \
    -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
[ -z "$PORTAL_HOST" ] && PORTAL_HOST=$($KUBECTL --namespace $helm_namespace \
    get services jarvice-mc-portal-lb \
    -o jsonpath='{.status.loadBalancer.ingress[0].ip}')

echo
echo "JARVICE API endpoint is available at:"
echo "https://$API_HOST/"
echo
echo "JARVICE portal is available at:"
echo "https://$PORTAL_HOST/"

