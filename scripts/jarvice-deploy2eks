#!/bin/bash

#set -x

if [ "$(arch)" != "x86_64" ]; then
    echo "This script only supports x86_64 architecture at the moment."
    exit 1
fi

eks_cluster_delete=
install_nvidia_plugin=
install_dashboard=
helm_name=jarvice
helm_namespace=jarvice-system
database_vol_delete=
vault_vols_delete=

jarvice_chart_dir=$(realpath $(dirname $0)/..)
config_file=$(realpath $(dirname $0)/eks-cluster.yaml)

function jarvice_deploy2eks_usage {
    cat <<EOF
Usage:
  $0 [global_options] [deploy_or_delete_options]

Available [global_options]:
  --jarvice-chart-dir <path>        Alternative JARVICE helm chart directory
                                    (Default: $jarvice_chart_dir)
  --config-file <filename>          Alternative cluster config file
                                    (Default: $config_file)

Available [delete_options]:
  --eks-cluster-delete              Delete the EKS cluster
  --database-vol-delete             Delete the database volume on cluster delete
  --vault-vols-delete               Delete the vault volumes on cluster delete

EOF
}
#Available [deploy_options]:
#  --install-dashboard               Install kubernetes dashboard into cluster


while [ $# -gt 0 ]; do
    case $1 in
        --help)
            jarvice_deploy2eks_usage
            exit 0
            ;;
        --jarvice-chart-dir)
            jarvice_chart_dir=$2
            shift; shift
            ;;
        --config-file)
            config_file=$2
            shift; shift
            ;;
        --install-dashboard)
            install_dashboard=y
            shift
            ;;
        --eks-cluster-delete)
            eks_cluster_delete=y
            shift
            ;;
        --database-vol-delete)
            database_vol_delete=y
            shift
            ;;
        --vault-vols-delete)
            vault_vols_delete=y
            shift
            ;;
        *)
            jarvice_deploy2eks_usage
            exit 1
            ;;
    esac
done

echo
echo "* Using cluster configuration file $config_file..."
if [ ! -f "$config_file" ]; then
    echo "Cannot find cluster config file $config_file.  Cannot continue."
    exit 1
fi


# If "Accelerate Computing" NVIDIA type found, auto install NVIDIA plugin
# See: https://aws.amazon.com/ec2/instance-types/
if [[ $eks_node_type == p2.* ]] || [[ $eks_node_type == p3.* ]]; then
    install_nvidia_plugin=y
fi

KUBECTL=$(type -p kubectl)
AWS=$(type -p aws)
AWS_IAM_AUTH=$(type -p aws-iam-authenticator)
EKSCTL=$(type -p eksctl)
HELM=$(type -p helm)
CURL=$(type -p curl)
UNZIP=$(type -p unzip)
JQ=$(type -p jq)
SHYAML=$(type -p shyaml)

KUBECTL_VER_REQ=1.10
EKSCTL_VER_REQ=0.1.22

if [ -z "$KUBECONFIG" ]; then
    KUBECONFIG=~/.kube/config
fi
export KUBECONFIG

###############################################################################

function check_deps {
    if [ -z "$KUBECTL" -o -z "$AWS" -o -z "$AWS_IAM_AUTH" -o -z "$EKSCTL" -o -z "$HELM" -o -z "$JQ" ]; then
        echo "Missing software needs to be installed.  Verifying sudo access..."
        SUID=$(sudo id -u)
        if [ "$SUID" != "0" ]; then
            echo "Could not verify sudo access.  Cannot continue."
            echo "Please resolve sudo access before re-running this script."
            exit 1
        else
            echo "Verified sudo access..."
        fi
        install_pkgs=
        if [ -z "$CURL" ]; then
            echo "curl command not found...  Installing..."
            install_pkgs+="curl "
        fi
        if [ -z "$JQ" ]; then
            echo "jq command not found...  Installing..."
            install_pkgs+="jq "
        fi
        if [ -z "$SHYAML" ]; then
            echo "shyaml command not found...  Installing..."
            sudo -H pip install shyaml
        fi
        if [ -n "$install_pkgs" ]; then
            if [ -e /etc/redhat-release ]; then
                sudo yum -y install $install_pkgs
            else
                sudo apt-get -y update
                sudo apt-get -y install $install_pkgs
            fi
        fi
    fi
}

function get_json_value {
    json=$1
    key=$2
    echo "$json" | \
        python -c "import json,sys;obj=json.load(sys.stdin);print obj$key;" \
        2>/dev/null || /bin/true
}

function get_yaml_value {
    key=$1
    cat $config_file | \
        $SHYAML -q get-value $key | grep -v ^None || /bin/true
}

function strict_version {
    ver=$1
    ver_req=$2
    python -c "from distutils.version import StrictVersion;print StrictVersion('$ver') >= StrictVersion('$ver_req');"
}

function kubectl_install_nvidia_plugin {
    echo
    echo "* Installing Nvidia device plugin into cluster..."
    SERVER_MAJOR=$($KUBECTL version | grep '^Server' | sed -E 's/.*Major:"([0-9]+)".*/\1/')
    SERVER_MINOR=$($KUBECTL version | grep '^Server' | sed -E 's/.*Minor:"([0-9]+)\+*".*/\1/')
    $KUBECTL create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v$SERVER_MAJOR.$SERVER_MINOR/nvidia-device-plugin.yml 2>/dev/null || /bin/true
}

function eks_cf_stack_delete {
    stack_name="$1"

    echo
    echo "* Deleting CloudFormation stack '$stack_name'..."
    $AWS --region $aws_region cloudformation \
        delete-stack --stack-name $stack_name
    status="DELETE_IN_PROGRESS"
    while [ "$status" = "DELETE_IN_PROGRESS" ]; do
        echo -n "."
        sleep 5
        status=$($AWS --region $aws_region cloudformation \
            describe-stacks --stack-name $stack_name --output=json \
            2>/dev/null || /bin/true)
        status=$(get_json_value "$status" "['Stacks'][0]['StackStatus']")
        status_reason=$(get_json_value "$status" "['Stacks'][0]['StackStatusReason']")
    done
    echo "$status"
    if [ -n "$status_reason" ]; then
        echo "$status_reason"
    fi
}

###############################################################################

# EKS config options
eks_cluster_name=$(get_yaml_value "metadata.name")
aws_region=$(get_yaml_value "metadata.region")
aws_zones=$(get_yaml_value "availabilityZones")

# JARVICE config options
registry_username=$(get_yaml_value "jarvice.imagePullSecret.username")
registry_password=$(get_yaml_value "jarvice.imagePullSecret.password")
jarvice_license=$(get_yaml_value "jarvice.JARVICE_LICENSE_LIC")
jarvice_username=$(get_yaml_value "jarvice.JARVICE_REMOTE_USER")
jarvice_apikey=$(get_yaml_value "jarvice.JARVICE_REMOTE_APIKEY")

if [ -n "$eks_cluster_delete" ]; then
    set -e
    check_deps

    echo
    echo "* Using $KUBECONFIG for kubeconfig..."

    echo
    echo "* Deleting EKS cluster '$eks_cluster_name' in region" \
        "'$aws_region'..."

    # eksctl doesn't delete EC2 load balancers and ELB security groups
    # associated with it.  So we're doing it the hard way..
    TAG_KEY_VAL="kubernetes.io/cluster/$eks_cluster_name"
    AWS_ELB_SEC_GROUP_NAMES=$($AWS --region $aws_region ec2 \
        describe-security-groups --no-paginate --output=text \
        --filters Name=tag-key,Values=$TAG_KEY_VAL \
        --query "SecurityGroups[*].{Name:GroupName}" | grep '^k8s-elb-' \
        || /bin/true)
    AWS_ELB_NAMES=$(echo "$AWS_ELB_SEC_GROUP_NAMES" | sed -e 's/k8s-elb-//')
    echo
    for elb_name in $AWS_ELB_NAMES; do
        echo "* Deleting AWS elastic load balancer '$elb_name'..."
        $AWS --region $aws_region elb delete-load-balancer \
            --load-balancer-name=$elb_name
    done

    echo
    echo "* Deleting EKS cluster '$eks_cluster_name' in region" \
        "'$aws_region'..."
    $EKSCTL delete cluster --wait --config-file $config_file

    AWS_ELB_SEC_GROUP_IDS=$($AWS --region $aws_region ec2 \
        describe-security-groups --no-paginate --output=text \
        --filters Name=tag-key,Values=$TAG_KEY_VAL \
        --query "SecurityGroups[*].{ID:GroupId}")

    echo
    for group_id in $AWS_ELB_SEC_GROUP_IDS; do
        echo "* Deleting AWS EC2 security group '$group_id'..."
        $AWS --region $aws_region ec2 delete-security-group \
            --group-id=$group_id
    done

    filters="Name=tag-key,Values=kubernetes.io/cluster/$eks_cluster_name"
    filters+=" Name=tag:kubernetes.io/created-for/pvc/namespace,Values=$helm_namespace-jobs"
    filters+=" Name=status,Values=available"
    vold_ids=$($AWS --region $aws_region ec2 \
        describe-volumes --no-paginate --output=json --filters $filters | \
        $JQ -r ".Volumes[].VolumeId | select(.!=null)")
    echo
    if [ -n "$vault_vols_delete" ]; then
        echo "* Deleting AWS EBS user vault volumes..."
    else
        echo "* Preserving AWS EBS user vault volumes..."
    fi
    for vol_id in $vol_ids; do
        echo "$vol_id"
        if [ -n "$vault_vols_delete" ]; then
            $AWS --region $aws_region ec2 \
                delete-volume --volume-id $vol_id
        fi
    done

    filters="Name=tag-key,Values=kubernetes.io/cluster/$eks_cluster_name"
    filters+=" Name=tag:kubernetes.io/created-for/pvc/namespace,Values=$helm_namespace"
    filters+=" Name=tag:kubernetes.io/created-for/pvc/name,Values=jarvice-db-pvc"
    filters+=" Name=status,Values=available"
    pvc_vol=$($AWS --region $aws_region ec2 \
        describe-volumes --no-paginate --output=json --filters $filters)
    vol_id=$(echo "$pvc_vol" | $JQ -r ".Volumes[0].VolumeId | select(.!=null)")
    if [ -z "$vol_id" ]; then
        echo
        echo "* ERROR: Could not find EBS volume containing JARVICE database!"
        echo
        exit 1
    fi
    if [ -n "$database_vol_delete" ]; then
        echo
        echo "* Deleting EBS volume ('$vol_id') containing JARVICE database..."
        $AWS --region $aws_region ec2 \
            delete-volume --volume-id $vol_id
    fi

    echo
    echo "------------------------------------------------------------------------"
    echo
    echo "Succesfully deleted '$eks_cluster_name' cluster is in the '$aws_region' region."
    echo
    if [ -z "$database_vol_delete" ]; then
        echo
        echo "** Preserved EBS volume containing the JARVICE database:  $vol_id"
        echo
        echo "* EBS volume for the JARVICE database may be deleted by executing:"
        echo "\$ $0 --eks-cluster-delete --config-file $config_file --database-vol-delete"
    fi
    if [ -z "$vault_vols_delete" ]; then
        echo
        echo "** Preserved EBS volumes containing user vaults."
        echo
        echo "* EBS user vault volumes may be deleted by executing:"
        echo "\$ $0 --eks-cluster-delete --config-file $config_file --vault-vols-delete"
    fi
    if [ -z "$database_vol_delete" -o -z "$vault_vols_delete" ]; then
        echo
        echo "** Preserved volumes will be reused if another '$eks_cluster_name' cluster is created in the '$aws_region' region."
    fi
    echo
    echo "It may take a few more minutes for the deletion to complete."
    echo "Status can be viewed here:"
    echo "https://us-west-2.console.aws.amazon.com/cloudformation/home#/stacks"
    echo
    exit 0
fi

if [ -z "$registry_username" -o -z "$registry_password" -o -z "$jarvice_license" -o -z "$jarvice_username" -o -z "$jarvice_apikey" ]; then
    echo "Missing credentials and/or license.  Cannot continue."
    echo "Please update the configuration file: $config_file"
    jarvice_deploy2eks_usage
fi

# Assumes helm chart basedir is one level up from this script
echo "Verifying helm chart directory '$jarvice_chart_dir'..."
CHART_NAME=$(grep '^name: jarvice' $jarvice_chart_dir/Chart.yaml 2>/dev/null)
if [ -z "$CHART_NAME" ]; then
    echo "Could not verify helm chart for JARVICE.  Cannot continue."
    echo "Use --jarvice-chart-dir to specify valid JARVICE helm chart" \
        "directory."
    exit 1
else
    echo "Found valid chart directory..."
fi

set -e
check_deps

[ -z "$INSTALL_DIR" ] && INSTALL_DIR=/usr/local

if [ -z "$KUBECTL" ]; then
    KUBECTL=$INSTALL_DIR/bin/kubectl
    echo "kubectl command not found...  Installing to $KUBECTL..."
    sudo bash -c "curl --silent --location https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl >$KUBECTL"
    sudo chown root.root $KUBECTL
    sudo chmod 755 $KUBECTL
else
    KUBECTL_MAJOR=$($KUBECTL version | grep '^Client' | sed -E 's/.*Major:"([0-9]+)".*/\1/')
    KUBECTL_MINOR=$($KUBECTL version | grep '^Client' | sed -E 's/.*Minor:"([0-9]+)\+*".*/\1/')
    KUBECTL_VER="$KUBECTL_MAJOR.$KUBECTL_MINOR"
    echo "Found $KUBECTL... Version: $KUBECTL_VER"
    if [ "$(strict_version $KUBECTL_VER $KUBECTL_VER_REQ)" != "True" ]; then
        echo "kubectl version $KUBECTL_VER_REQ or newer is required."
        echo "Please upgrade kubectl or remove it."
        echo "This script will re-install kubectl if it is removed."
        exit 1
    fi
fi

if [ -z "$AWS" ]; then
    AWS=$INSTALL_DIR/bin/aws
    echo "aws command not found...  Installing to $AWS..."
    if [ -z "$UNZIP" ]; then
        echo "unzip command not found...  Installing..."
        if [ -e /etc/redhat-release ]; then
            sudo yum -y install unzip
        else
            sudo apt-get -y update
            sudo apt-get -y install unzip
        fi
    fi
    curl --silent --location "https://s3.amazonaws.com/aws-cli/awscli-bundle.zip" >/tmp/awscli-bundle.zip
    unzip -d /tmp /tmp/awscli-bundle.zip >/dev/null
    sudo /tmp/awscli-bundle/install -i $INSTALL_DIR/aws -b $AWS
    rm -rf /tmp/awscli-bundle*
else
    echo "Found $AWS..."
fi

AWS_IAM_USER=$($AWS --output=json iam get-user | \
    $JQ -r ".User.UserName | select(.!=null)")
if [ -z "$AWS_IAM_USER" ]; then
    echo "AWS IAM user not found..."
    echo "Please set/export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables: https://docs.aws.amazon.com/cli/latest/userguide/cli-environment.html"
    echo "Or set aws_access_key_id and aws_secret_access_key in your credentials file: https://docs.aws.amazon.com/cli/latest/userguide/cli-config-files.html"
    exit 1
else
    echo "Found AWS IAM user... User name: $AWS_IAM_USER"
fi

if [ -z "$AWS_IAM_AUTH" ]; then
    AWS_IAM_AUTH=$INSTALL_DIR/bin/aws-iam-authenticator
    echo "aws-iam-authenticator command not found...  Installing to $AWS_IAM_AUTH..."
    sudo bash -c "curl --silent --location https://amazon-eks.s3-us-west-2.amazonaws.com/1.10.3/2018-07-26/bin/linux/amd64/aws-iam-authenticator >$AWS_IAM_AUTH"
    sudo chown root.root $AWS_IAM_AUTH
    sudo chmod 755 $AWS_IAM_AUTH
else
    echo "Found $AWS_IAM_AUTH..."
fi

if [ -z "$EKSCTL" ]; then
    EKSCTL=$INSTALL_DIR/bin/eksctl
    echo "eksctl command not found...  Installing to $EKSCTL..."
    curl --silent --location "https://github.com/weaveworks/eksctl/releases/download/latest_release/eksctl_$(uname -s)_amd64.tar.gz" | sudo tar xz -C /usr/local/bin
    sudo chown root.root $EKSCTL
    sudo chmod 755 $EKSCTL
else
    EKSCTL_VER=$($EKSCTL version)
    EKSCTL_VER=$(echo "$EKSCTL_VER" | sed -e 's/.*{/{/')
    EKSCTL_VER=$(echo "$EKSCTL_VER" | sed -e 's/.*GitTag:"\(.*\)".*/\1/' | \
        awk -F- '{print $1}')
    echo "Found $EKSCTL... Version: $EKSCTL_VER"

    if [ "$(strict_version $EKSCTL_VER $EKSCTL_VER_REQ)" != "True" ]; then
        echo "eksctl version $EKSCTL_VER_REQ or newer is required."
        echo "Please upgrade eksctl or remove it."
        echo "This script will re-install eksctl if it is removed."
        exit 1
    fi
fi

if [ -z "$HELM" ]; then
    HELM_INSTALL_DIR=$INSTALL_DIR/bin
    HELM=$HELM_INSTALL_DIR/helm
    echo "helm command not found...  Installing to $HELM..."
    curl --silent https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get >/tmp/get_helm.sh
    sudo --preserve-env=PATH,HELM_INSTALL_DIR bash /tmp/get_helm.sh >/dev/null
    rm -f /tmp/get_helm.sh
else
    echo "Found $HELM..."
fi

echo "------------------------------------------------------------------------"

echo
echo "* Using $KUBECONFIG for kubeconfig..."
OLD_CONTEXT=$($KUBECTL config current-context 2>/dev/null || /bin/true)

echo
echo "* Running $EKSCTL to create EKS cluster..."
$EKSCTL create cluster --config-file $config_file

echo
echo "* Labeling and tainting nodes..."
roles="jarvice-system jarvice-dockerbuild jarvice-dockerpull jarvice-compute"
for role in $roles; do
    $KUBECTL label nodes -l node-role.jarvice.io/$role= \
        node-role.kubernetes.io/$role=
    $KUBECTL taint nodes -l node-role.kubernetes.io/$role= \
        node-role.kubernetes.io/$role=:NoSchedule
done

#if [ -n "$install_weave_plugin" ]; then
#    echo
#    echo "* Installing Weave Net network plugin into cluster..."
#    $KUBECTL apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
#fi

JARVICE_MACHINES_ADD='[{"mc_name":"n0", "mc_description":"2 core, 16GB RAM (CPU only)", "mc_cores":"2", "mc_slots":"16", "mc_gpus":"0", "mc_ram":"16", "mc_swap":"8", "mc_scratch":"64", "mc_devices":"", "mc_properties":"", "mc_slave_properties":"", "mc_slave_gpus":"0", "mc_slave_ram":"16", "mc_scale_min":"1", "mc_scale_max":"1", "mc_scale_select":"", "mc_lesser":"1", "mc_price":"0.00", "mc_priority":"0", "mc_privs":"", "mc_arch":"x86_64"}'
JARVICE_MACHINES_ADD+=', {"mc_name":"n1", "mc_description":"4 core, 32GB RAM (CPU Only)", "mc_cores":"4", "mc_slots":"16", "mc_gpus":"0", "mc_ram":"32", "mc_swap":"16", "mc_scratch":"100", "mc_devices":"", "mc_properties":"", "mc_slave_properties":"", "mc_slave_gpus":"0", "mc_slave_ram":"32", "mc_scale_min":"1", "mc_scale_max":"1", "mc_scale_select":"", "mc_lesser":"1", "mc_price":"0.00", "mc_priority":"0", "mc_privs":"", "mc_arch":"x86_64"}'
JARVICE_MACHINES_ADD+=', {"mc_name":"n3", "mc_description":"16 core, 128GB RAM (CPU Only)", "mc_cores":"16", "mc_slots":"16", "mc_gpus":"0", "mc_ram":"128", "mc_swap":"64", "mc_scratch":"500", "mc_devices":"", "mc_properties":"", "mc_slave_properties":"", "mc_slave_gpus":"0", "mc_slave_ram":"128", "mc_scale_min":"1", "mc_scale_max":"256", "mc_scale_select":"", "mc_lesser":"1", "mc_price":"0.00", "mc_priority":"0", "mc_privs":"", "mc_arch":"x86_64"}'
if [ -n "$install_nvidia_plugin" ]; then
    kubectl_install_nvidia_plugin
    JARVICE_MACHINES_ADD+=', {"mc_name":"ng0", "mc_description":"2 core, 16GB RAM with Nvidia GPU", "mc_cores":"2", "mc_slots":"16", "mc_gpus":"1", "mc_ram":"16", "mc_swap":"8", "mc_scratch":"64", "mc_devices":"", "mc_properties":"", "mc_slave_properties":"", "mc_slave_gpus":"0", "mc_slave_ram":"16", "mc_scale_min":"1", "mc_scale_max":"1", "mc_scale_select":"", "mc_lesser":"1", "mc_price":"0.00", "mc_priority":"0", "mc_privs":"", "mc_arch":"x86_64"}'
    JARVICE_MACHINES_ADD+=', {"mc_name":"ng1", "mc_description":"4 core, 32GB RAM with Nvidia GPU", "mc_cores":"4", "mc_slots":"16", "mc_gpus":"1", "mc_ram":"32", "mc_swap":"16", "mc_scratch":"100", "mc_devices":"", "mc_properties":"", "mc_slave_properties":"", "mc_slave_gpus":"0", "mc_slave_ram":"32", "mc_scale_min":"1", "mc_scale_max":"1", "mc_scale_select":"", "mc_lesser":"1", "mc_price":"0.00", "mc_priority":"0", "mc_privs":"", "mc_arch":"x86_64"}'
    JARVICE_MACHINES_ADD+=', {"mc_name":"ng3", "mc_description":"16 core, 128GB RAM with Nvidia GPU", "mc_cores":"16", "mc_slots":"16", "mc_gpus":"1", "mc_ram":"128", "mc_swap":"64", "mc_scratch":"500", "mc_devices":"", "mc_properties":"", "mc_slave_properties":"", "mc_slave_gpus":"1", "mc_slave_ram":"128", "mc_scale_min":"1", "mc_scale_max":"2", "mc_scale_select":"", "mc_lesser":"1", "mc_price":"0.00", "mc_priority":"0", "mc_privs":"", "mc_arch":"x86_64"}'
fi
JARVICE_MACHINES_ADD+=']'

echo
echo "* Initializing helm and installing Tiller into cluster..."
$HELM init

echo
echo "* Creating/setting service account and cluster role binding for Tiller..."
tiller_sa_yaml=$(cat <<EOF
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller
    namespace: kube-system
EOF
)
echo "$tiller_sa_yaml" | $KUBECTL --namespace kube-system create -f -
$HELM init --upgrade --service-account tiller
$KUBECTL -n kube-system patch deployment tiller-deploy \
    -p '{"spec": {"template":{"spec":{"tolerations":[{"key": "node-role.kubernetes.io/jarvice-system", "effect": "NoSchedule", "operator": "Exists"}]}}}}'
sleep 1

echo
echo -n "* "
$KUBECTL --namespace kube-system rollout status \
    deployment tiller-deploy --watch=true

if [ -n "$install_dashboard" ]; then
    echo
    echo "* Installing kubernetes-dashboard..."
    $HELM install \
        --set tolerations[0].effect=NoSchedule \
        --set tolerations[0].key=node-role.kubernetes.io/jarvice-system \
        --set tolerations[0].operator=Exists \
        --namespace kube-system \
        --name kubernetes-dashboard \
        stable/kubernetes-dashboard
    $KUBECTL --namespace kube-system expose deployment kubernetes-dashboard \
        --type=LoadBalancer --name kubernetes-dashboard-lb
fi

echo
echo "* Creating 'jarvice-db' StorageClass..."
storage_class_yaml=$(cat <<EOF
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: jarvice-db
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2
reclaimPolicy: Retain
mountOptions:
  - debug
EOF
)
echo "$storage_class_yaml" | $KUBECTL --namespace kube-system create -f -

filters="Name=tag-key,Values=kubernetes.io/cluster/$eks_cluster_name"
filters+=" Name=tag:kubernetes.io/created-for/pvc/namespace,Values=$helm_namespace"
filters+=" Name=tag:kubernetes.io/created-for/pvc/name,Values=jarvice-db-pvc"
filters+=" Name=status,Values=available"
echo
echo "* Searching for 'jarvice-db-pvc' EBS volume previously created for '$eks_cluster_name'..."
pvc_vol=$($AWS --region $aws_region ec2 \
        describe-volumes --no-paginate --output=json --filters $filters)
vol_id=$(echo "$pvc_vol" | $JQ -r ".Volumes[0].VolumeId | select(.!=null)")
if [ -n "$vol_id" ]; then
    vol_size=$(echo "$pvc_vol" | $JQ -r ".Volumes[0].Size")
    vol_zone=$(echo "$pvc_vol" | $JQ -r ".Volumes[0].AvailabilityZone")
    echo "Found ${vol_size}GB volume '$vol_id' in availability zone '$vol_zone'..."
    echo
    echo "* Recreating 'jarvice-db-pv' PersistentVolume using '$vol_id'..."
    pv_yaml=$(cat <<EOF
apiVersion: v1
kind: PersistentVolume
metadata:
  name: jarvice-db-pv
spec:
  awsElasticBlockStore:
    fsType: ext4
    volumeID: aws://$vol_zone/$vol_id
  capacity:
    storage: ${vol_size}Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: jarvice-db
  mountOptions:
    - debug
EOF
)
    echo "$pv_yaml" | $KUBECTL --namespace kube-system create -f -
else
    echo "No previous EBS volume was found."
fi

jarvice_jobs_domain=$(get_yaml_value "jarvice.JARVICE_JOBS_DOMAIN")
jarvice_jobs_lb_service=$(get_yaml_value "jarvice.JARVICE_JOBS_LB_SERVICE")
echo
echo "* Running helm to deploy JARVICE into cluster..."
$HELM install --debug \
    --set jarvice.imagePullSecret.username="$registry_username" \
    --set jarvice.imagePullSecret.password="$registry_password" \
    --set jarvice.JARVICE_LICENSE_LIC="$jarvice_license" \
    --set jarvice.JARVICE_REMOTE_USER="$jarvice_username" \
    --set jarvice.JARVICE_REMOTE_APIKEY="$jarvice_apikey" \
    --set jarvice.JARVICE_JOBS_DOMAIN="$jarvice_jobs_domain" \
    --set jarvice.JARVICE_JOBS_LB_SERVICE="$jarvice_jobs_lb_service" \
    --set jarvice_db.persistence.enabled="true" \
    --set jarvice_dal.env.JARVICE_MACHINES_ADD="$(echo "$JARVICE_MACHINES_ADD" | sed -e 's#\([]{}",[/]\)#\\\1#g')" \
    --name $helm_name --namespace $helm_namespace $jarvice_chart_dir
sleep 1

echo
echo "* Installing LXCFS daemonset..."
$KUBECTL --namespace kube-system create \
    -f https://raw.githubusercontent.com/nimbix/lxcfs-initializer/master/lxcfs-daemonset.yaml

echo
echo "* Installing Traefik ingress controller..."
cert_file=$(get_yaml_value "jarvice.ingress.cert_file")
key_file=$(get_yaml_value "jarvice.ingress.key_file")
traefik_lb_ip=$(get_yaml_value "jarvice.ingress.loadBalancerIP")

[ -n "$cert_file" ] && default_cert="$(cat $cert_file | base64 -w 0)" && \
    default_cert="--set ssl.defaultCert=$default_cert"
[ -n "$key_file" ] && default_key="$(cat $key_file | base64 -w 0)" && \
    default_key="--set ssl.defaultKey=$default_key"
[ -n "$traefik_lb_ip" ] && lb_ip="--set loadBalancerIP=$traefik_lb_ip"

$HELM install --debug \
    --set nodeSelector."node-role\.kubernetes\.io/jarvice-system"="" \
    --set tolerations[0]."key"="node-role\.kubernetes\.io/jarvice-system" \
    --set tolerations[0]."effect"="NoSchedule" \
    --set tolerations[0]."operator"="Exists" \
    --set rbac.enabled=true \
    --set ssl.enabled=true \
    --set ssl.enforced=true \
    --set ssl.permanentRedirect=true \
    --set ssl.insecureSkipVerify=true \
    --set replicas=3 \
    --set memoryRequest=1Gi --set memoryLimit=1Gi \
    --set cpuRequest=1 --set cpuLimit=1 \
    $default_cert $default_key $lb_ip \
    --namespace kube-system --name traefik stable/traefik
#    --set dashboard.enabled=true \
#    --set dashboard.domain=traefik-dashboard.<domain> \

filters="Name=tag-key,Values=kubernetes.io/cluster/$eks_cluster_name"
filters+=" Name=tag:kubernetes.io/created-for/pvc/namespace,Values=$helm_namespace-jobs"
filters+=" Name=status,Values=available"
echo
echo "* Searching for EBS user vault volumes previously created for '$eks_cluster_name'..."
vol_ids=$($AWS --region $aws_region ec2 \
    describe-volumes --no-paginate --output=json --filters $filters | \
    $JQ -r ".Volumes[].VolumeId | select(.!=null)")
for vol_id in $vol_ids; do
    pvc_vol=$($AWS --region $aws_region ec2 \
        describe-volumes --no-paginate --output=json --volume-ids $vol_id)
    vol_type=$(echo "$pvc_vol" | $JQ -r ".Volumes[0].VolumeType")
    vol_size=$(echo "$pvc_vol" | $JQ -r ".Volumes[0].Size")
    vol_zone=$(echo "$pvc_vol" | $JQ -r ".Volumes[0].AvailabilityZone")
    vol_pv_name=$(echo "$pvc_vol" | $JQ -r ".Volumes[0].Tags[] | select(.Key==\"kubernetes.io/created-for/pvc/name\").Value")
    echo "Found ${vol_size}GB volume '$vol_id' in availability zone '$vol_zone'..."
    echo
    echo "* Recreating PersistentVolume '$vol_pv_name' using '$vol_id' for cluster '$eks_cluster_name'..."
    pv_yaml=$(cat <<EOF
apiVersion: v1
kind: PersistentVolume
metadata:
  name: $vol_pv_name
spec:
  awsElasticBlockStore:
    fsType: ext4
    volumeID: aws://$vol_zone/$vol_id
  capacity:
    storage: ${vol_size}Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: $vol_type
  mountOptions:
    - debug
EOF
)
    echo "$pv_yaml" | $KUBECTL --namespace kube-system create -f -

    echo
    echo "* Recreating PersistentVolumeClaim '$vol_pv_name' in '$helm_namespace-jobs' namespace..."
    pvc_yaml=$(cat <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: $vol_pv_name
spec:
  resources:
    requests:
      storage: ${vol_size}Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  storageClassName: $vol_type
EOF
)
    echo "$pvc_yaml" | $KUBECTL --namespace $helm_namespace-jobs create -f -
done

if [ -z "$vol_ids" ]; then
    echo "No previous EBS user vault volumes were found."
fi

DEPLOYMENTS="jarvice-dal jarvice-scheduler jarvice-api"
DEPLOYMENTS+=" jarvice-mc-portal jarvice-appsync"
for deployment in $DEPLOYMENTS; do
    echo
    echo -n "* "
    $KUBECTL --namespace $helm_namespace rollout status \
        deployment $deployment --watch=true
done

echo
echo "------------------------------------------------------------------------"

echo
echo "JARVICE is deployed and ready!"
echo
echo "This EKS cluster may be deleted by executing:"
echo "\$ $0 --eks-cluster-delete --config-file $config_file"

if [ -n "$OLD_CONTEXT" ]; then
    echo
    echo "kubeconfig file ($KUBECONFIG) has been modified."
    echo "Revert to the previous kubeconfig current-context by executing:"
    echo "\$ kubectl config set current-context $OLD_CONTEXT"
fi

API_HOST=$($KUBECTL --namespace $helm_namespace \
    describe services jarvice-api-lb | \
    grep Ingress | awk '{print $3}')
PORTAL_HOST=$($KUBECTL --namespace $helm_namespace \
    describe services jarvice-mc-portal-lb | \
    grep Ingress | awk '{print $3}')

if [ -n "$install_dashboard" ]; then
    DASHBOARD_HOST=$($KUBECTL --namespace $helm_namespace \
        describe services kubernetes-dashboard-lb | \
        grep Ingress | awk '{print $3}')
    echo
    echo "Kubernetes dashbaord is available at:"
    echo "https://$DASHBOARD_HOST:8443/"
fi
TRAEFIK_HOST=$($KUBECTL --namespace kube-system \
    describe services traefik | \
    grep Ingress | awk '{print $3}')


echo
echo "JARVICE API endpoint is available at:"
echo "https://$API_HOST/"
echo
echo "JARVICE portal is available at:"
echo "https://$PORTAL_HOST/"
echo
echo "Optionally, set up DNS for Traefik ingress available at:"
echo "$TRAEFIK_HOST"
echo
echo "Use '$EKSCTL get/scale/create/delete nodegroup --cluster $eks_cluster_name' to modify cluster nodes."
echo

