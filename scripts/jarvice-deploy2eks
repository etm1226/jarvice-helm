#!/bin/bash

#set -x

if [ "$(arch)" != "x86_64" ]; then
    echo "This script only supports x86_64 architecture at the moment."
    exit 1
fi

eks_cluster_delete=
install_nvidia_plugin=
helm_name=jarvice
helm_namespace=jarvice-system
database_vol_delete=
vault_vols_delete=

jarvice_chart_dir=$(realpath $(dirname $0)/..)
config_file=$(realpath $(dirname $0)/eks-cluster.yaml)

function jarvice_deploy2eks_usage {
    cat <<EOF
Usage:
  $0 [global_options] [deploy_or_delete_options]

Available [global_options]:
  --jarvice-chart-dir <path>        Alternative JARVICE helm chart directory
                                    (Default: $jarvice_chart_dir)
  --config-file <filename>          Alternative cluster config file
                                    (Default: $config_file)

Available [delete_options]:
  --cluster-delete                  Delete the EKS cluster
  --database-vol-delete             Delete the database volume on cluster delete
  --vault-vols-delete               Delete the vault volumes on cluster delete

EOF
}


while [ $# -gt 0 ]; do
    case $1 in
        --help)
            jarvice_deploy2eks_usage
            exit 0
            ;;
        --jarvice-chart-dir)
            jarvice_chart_dir=$2
            shift; shift
            ;;
        --config-file)
            config_file=$2
            shift; shift
            ;;
        --cluster-delete|--eks-cluster-delete)
            eks_cluster_delete=y
            shift
            ;;
        --database-vol-delete)
            database_vol_delete=y
            shift
            ;;
        --vault-vols-delete)
            vault_vols_delete=y
            shift
            ;;
        *)
            jarvice_deploy2eks_usage
            exit 1
            ;;
    esac
done

echo
echo "* Using cluster configuration file $config_file..."
if [ ! -f "$config_file" ]; then
    echo "Cannot find cluster config file $config_file.  Cannot continue."
    exit 1
fi


# If "Accelerate Computing" NVIDIA type found, auto install NVIDIA plugin
# See: https://aws.amazon.com/ec2/instance-types/
if [[ $eks_node_type == p2.* ]] || [[ $eks_node_type == p3.* ]]; then
    install_nvidia_plugin=y
fi

[ -z "$KUBECTL" ] && KUBECTL=$(type -p kubectl)
[ -z "$AWS" ] && AWS=$(type -p aws)
[ -z "$AWS_IAM_AUTH" ] && AWS_IAM_AUTH=$(type -p aws-iam-authenticator)
[ -z "$EKSCTL" ] && EKSCTL=$(type -p eksctl)
[ -z "$HELM" ] && HELM=$(type -p helm)
[ -z "$CURL" ] && CURL=$(type -p curl)
[ -z "$UNZIP" ] && UNZIP=$(type -p unzip)
[ -z "$JQ" ] && JQ=$(type -p jq)
[ -z "$YQ" ] && YQ=$(type -p yq)

HELM_VER_REQ=3.0.0
KUBECTL_VER_REQ=1.10
EKSCTL_VER_REQ=0.7.0

if [ -z "$KUBECONFIG" ]; then
    KUBECONFIG=~/.kube/config
fi
export KUBECONFIG

###############################################################################

function check_deps {
    if [ -z "$KUBECTL" -o -z "$AWS" -o -z "$AWS_IAM_AUTH" -o -z "$EKSCTL" -o -z "$HELM" -o -z "$JQ" -o -z "$YQ" ]; then
        echo "Missing software needs to be installed.  Verifying sudo access..."
        SUID=$(sudo id -u)
        if [ "$SUID" != "0" ]; then
            echo "Could not verify sudo access.  Cannot continue."
            echo "Please resolve sudo access before re-running this script."
            exit 1
        else
            echo "Verified sudo access..."
        fi
        install_pkgs=
        if [ -z "$CURL" ]; then
            echo "curl command not found...  Installing..."
            install_pkgs+="curl "
        fi
        if [ -z "$JQ" ]; then
            echo "jq command not found...  Installing..."
            install_pkgs+="jq "
        fi
        if [ -z "$YQ" ]; then
            echo "yq command not found...  Installing..."
            sudo curl --silent --location -o $INSTALL_DIR/bin/yq https://github.com/mikefarah/yq/releases/download/2.4.0/yq_linux_amd64
            sudo chmod 755 $INSTALL_DIR/bin/yq
        fi
        if [ -n "$install_pkgs" ]; then
            if [ -e /etc/redhat-release ]; then
                sudo yum -y install $install_pkgs
            else
                sudo apt-get -y update
                sudo apt-get -y install $install_pkgs
            fi
        fi
    fi
}

function get_json_value {
    json=$1
    key=$2
    echo "$json" | \
        python -c "import json,sys;obj=json.load(sys.stdin);print obj$key;" \
        2>/dev/null || /bin/true
}

function get_yaml_value {
    yaml_file=$1
    key=$2
    $YQ r $yaml_file $key | grep -v ^null || /bin/true
}

function strict_version {
    ver=$1
    ver_req=$2
    python -c "from distutils.version import StrictVersion;print StrictVersion('$ver') >= StrictVersion('$ver_req');" 2>/dev/null || /bin/true
}

function kubectl_install_nvidia_plugin {
    echo
    echo "* Installing Nvidia device plugin into cluster..."
    SERVER_MAJOR=$($KUBECTL version | grep '^Server' | sed -E 's/.*Major:"([0-9]+)".*/\1/')
    SERVER_MINOR=$($KUBECTL version | grep '^Server' | sed -E 's/.*Minor:"([0-9]+)\+*".*/\1/')
    $KUBECTL create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v$SERVER_MAJOR.$SERVER_MINOR/nvidia-device-plugin.yml 2>/dev/null || /bin/true
}

###############################################################################

# EKS config options
eks_cluster_name=$(get_yaml_value $config_file "metadata.name")
aws_region=$(get_yaml_value $config_file "metadata.region")
aws_zones=$(get_yaml_value $config_file "availabilityZones")

# JARVICE config options
jarvice_image_pull_secret=$(get_yaml_value $config_file "jarvice.imagePullSecret")
jarvice_license=$(get_yaml_value $config_file "jarvice.JARVICE_LICENSE_LIC")
jarvice_username=$(get_yaml_value $config_file "jarvice.JARVICE_REMOTE_USER")
jarvice_apikey=$(get_yaml_value $config_file "jarvice.JARVICE_REMOTE_APIKEY")
jarvice_cluster_type=$(get_yaml_value $config_file "jarvice.JARVICE_CLUSTER_TYPE")
jarvice_sched_server_key=$(get_yaml_value $config_file "jarvice.JARVICE_SCHED_SERVER_KEY")

if [ -n "$eks_cluster_delete" ]; then
    set -e
    check_deps

    echo
    echo "* Using $KUBECONFIG for kubeconfig..."

    echo
    echo "* Deleting EKS cluster '$eks_cluster_name' in region" \
        "'$aws_region'..."
    $YQ d $config_file jarvice | $EKSCTL delete cluster --wait --config-file -

    filters="Name=tag-key,Values=kubernetes.io/cluster/$eks_cluster_name"
    filters+=" Name=tag:kubernetes.io/created-for/pvc/namespace,Values=$helm_namespace-jobs"
    filters+=" Name=status,Values=available"
    vold_ids=$($AWS --region $aws_region ec2 \
        describe-volumes --no-paginate --output=json --filters $filters | \
        $JQ -r ".Volumes[].VolumeId | select(.!=null)")
    echo
    if [ -n "$vault_vols_delete" ]; then
        echo "* Deleting AWS EBS user vault volumes..."
    else
        echo "* Preserving AWS EBS user vault volumes..."
    fi
    for vol_id in $vol_ids; do
        echo "$vol_id"
        if [ -n "$vault_vols_delete" ]; then
            $AWS --region $aws_region ec2 \
                delete-volume --volume-id $vol_id
        fi
    done

    if [ "$jarvice_cluster_type" != "downstream" ]; then
        filters="Name=tag-key,Values=kubernetes.io/cluster/$eks_cluster_name"
        filters+=" Name=tag:kubernetes.io/created-for/pvc/namespace,Values=$helm_namespace"
        filters+=" Name=tag:kubernetes.io/created-for/pvc/name,Values=jarvice-db-pvc"
        filters+=" Name=status,Values=available"
        pvc_vol=$($AWS --region $aws_region ec2 \
            describe-volumes --no-paginate --output=json --filters $filters)
        vol_id=$(echo "$pvc_vol" | $JQ -r ".Volumes[0].VolumeId | select(.!=null)")
        if [ -z "$vol_id" ]; then
            echo
            echo "* ERROR: Could not find EBS volume containing JARVICE database!"
            echo
            exit 1
        fi
        if [ -n "$database_vol_delete" ]; then
            echo
            echo "* Deleting EBS volume ('$vol_id') containing JARVICE database..."
            $AWS --region $aws_region ec2 \
                delete-volume --volume-id $vol_id
        fi
    fi

    echo
    echo "------------------------------------------------------------------------"
    echo
    echo "Succesfully deleted '$eks_cluster_name' cluster is in the '$aws_region' region."
    echo
    if [ "$jarvice_cluster_type" != "downstream" ]; then
        if [ -z "$database_vol_delete" ]; then
            echo
            echo "** Preserved EBS volume containing the JARVICE database:  $vol_id"
            echo
            echo "* EBS volume for the JARVICE database may be deleted by executing:"
            echo "\$ $0 --eks-cluster-delete --config-file $config_file --database-vol-delete"
        fi
    fi
    if [ -z "$vault_vols_delete" ]; then
        echo
        echo "** Preserved EBS volumes containing user vaults."
        echo
        echo "* EBS user vault volumes may be deleted by executing:"
        echo "\$ $0 --eks-cluster-delete --config-file $config_file --vault-vols-delete"
    fi
    if [ -z "$database_vol_delete" -o -z "$vault_vols_delete" ]; then
        echo
        echo "** Preserved volumes will be reused if another '$eks_cluster_name' cluster is created in the '$aws_region' region."
    fi
    echo
    echo "It may take a few more minutes for the deletion to complete."
    echo "Status can be viewed here:"
    echo "https://us-west-2.console.aws.amazon.com/cloudformation/home#/stacks"
    echo
    exit 0
fi

if [ "$jarvice_cluster_type" = "downstream" ] && [ -z "$jarvice_image_pull_secret" ]; then
    echo "Missing credentials and/or license.  Cannot continue."
    echo "Please update the configuration file: $config_file"
    jarvice_deploy2eks_usage
    exit 1
elif [ "$jarvice_cluster_type" != "downstream" ] && [ -z "$jarvice_image_pull_secret" -o -z "$jarvice_license" -o -z "$jarvice_username" -o -z "$jarvice_apikey" ]; then
    echo "Missing credentials and/or license.  Cannot continue."
    echo "Please update the configuration file: $config_file"
    jarvice_deploy2eks_usage
    exit 1
fi

# Assumes helm chart basedir is one level up from this script
echo "Verifying helm chart directory '$jarvice_chart_dir'..."
CHART_NAME=$(grep '^name: jarvice' $jarvice_chart_dir/Chart.yaml 2>/dev/null)
if [ -z "$CHART_NAME" ]; then
    echo "Could not verify helm chart for JARVICE.  Cannot continue."
    echo "Use --jarvice-chart-dir to specify valid JARVICE helm chart" \
        "directory."
    exit 1
else
    echo "Found valid chart directory..."
fi

set -e
check_deps

[ -z "$INSTALL_DIR" ] && INSTALL_DIR=/usr/local

if [ -z "$KUBECTL" ]; then
    KUBECTL=$INSTALL_DIR/bin/kubectl
    echo "kubectl command not found...  Installing to $KUBECTL..."
    sudo bash -c "curl --silent --location https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl >$KUBECTL"
    sudo chown root.root $KUBECTL
    sudo chmod 755 $KUBECTL
else
    KUBECTL_MAJOR=$($KUBECTL version 2>/dev/null | grep '^Client' | sed -E 's/.*Major:"([0-9]+)".*/\1/')
    KUBECTL_MINOR=$($KUBECTL version 2>/dev/null | grep '^Client' | sed -E 's/.*Minor:"([0-9]+)\+*".*/\1/')
    KUBECTL_VER="$KUBECTL_MAJOR.$KUBECTL_MINOR"
    echo "Found $KUBECTL... Version: $KUBECTL_VER"
    if [ "$(strict_version $KUBECTL_VER $KUBECTL_VER_REQ)" != "True" ]; then
        echo "kubectl version $KUBECTL_VER_REQ or newer is required."
        echo "Please upgrade kubectl or remove it."
        echo "This script will re-install kubectl if it is removed."
        exit 1
    fi
fi

if [ -z "$AWS" ]; then
    AWS=$INSTALL_DIR/bin/aws
    echo "aws command not found...  Installing to $AWS..."
    if [ -z "$UNZIP" ]; then
        echo "unzip command not found...  Installing..."
        if [ -e /etc/redhat-release ]; then
            sudo yum -y install unzip
        else
            sudo apt-get -y update
            sudo apt-get -y install unzip
        fi
    fi
    curl --silent --location "https://s3.amazonaws.com/aws-cli/awscli-bundle.zip" >/tmp/awscli-bundle.zip
    unzip -d /tmp /tmp/awscli-bundle.zip >/dev/null
    sudo /tmp/awscli-bundle/install -i $INSTALL_DIR/aws -b $AWS
    rm -rf /tmp/awscli-bundle*
else
    echo "Found $AWS..."
fi

AWS_IAM_USER=$($AWS --output=json iam get-user | \
    $JQ -r ".User.UserName | select(.!=null)")
if [ -z "$AWS_IAM_USER" ]; then
    echo "AWS IAM user not found..."
    echo "Please set/export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables: https://docs.aws.amazon.com/cli/latest/userguide/cli-environment.html"
    echo "Or set aws_access_key_id and aws_secret_access_key in your credentials file: https://docs.aws.amazon.com/cli/latest/userguide/cli-config-files.html"
    exit 1
else
    echo "Found AWS IAM user... User name: $AWS_IAM_USER"
fi

if [ -z "$AWS_IAM_AUTH" ]; then
    AWS_IAM_AUTH=$INSTALL_DIR/bin/aws-iam-authenticator
    echo "aws-iam-authenticator command not found...  Installing to $AWS_IAM_AUTH..."
    sudo bash -c "curl --silent --location https://amazon-eks.s3-us-west-2.amazonaws.com/1.10.3/2018-07-26/bin/linux/amd64/aws-iam-authenticator >$AWS_IAM_AUTH"
    sudo chown root.root $AWS_IAM_AUTH
    sudo chmod 755 $AWS_IAM_AUTH
else
    echo "Found $AWS_IAM_AUTH..."
fi

if [ -z "$EKSCTL" ]; then
    EKSCTL=$INSTALL_DIR/bin/eksctl
    echo "eksctl command not found...  Installing to $EKSCTL..."
    curl --silent --location "https://github.com/weaveworks/eksctl/releases/download/latest_release/eksctl_$(uname -s)_amd64.tar.gz" | sudo tar xz -C /usr/local/bin
    sudo chown root.root $EKSCTL
    sudo chmod 755 $EKSCTL
else
    EKSCTL_VER=$($EKSCTL version)
    EKSCTL_VER=$(echo "$EKSCTL_VER" | sed -e 's/.*{/{/')
    EKSCTL_VER=$(echo "$EKSCTL_VER" | sed -e 's/.*GitTag:"\(.*\)".*/\1/' | \
        awk -F- '{print $1}')
    echo "Found $EKSCTL... Version: $EKSCTL_VER"

    if [ "$(strict_version $EKSCTL_VER $EKSCTL_VER_REQ)" != "True" ]; then
        echo "eksctl version $EKSCTL_VER_REQ or newer is required."
        echo "Please upgrade eksctl or remove it."
        echo "This script will re-install eksctl if it is removed."
        exit 1
    fi
fi

if [ -z "$HELM" ]; then
    HELM_INSTALL_DIR=$INSTALL_DIR/bin
    HELM=$HELM_INSTALL_DIR/helm
    echo "helm command not found...  Installing to $HELM..."
    curl --silent https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get >/tmp/get_helm.sh
    sudo --preserve-env bash /tmp/get_helm.sh >/dev/null
    rm -f /tmp/get_helm.sh
else
    HELM_VER=$($HELM version 2>/dev/null | grep 'Version:' | sed -E 's/.*Version:"v([.0-9]+)".*/\1/')
    echo "Found $HELM... Version: $HELM_VER"
    if [ "$(strict_version $HELM_VER $HELM_VER_REQ)" != "True" ]; then
        echo "helm version $HELM_VER_REQ or newer is required."
        echo "Please upgrade helm or remove it."
        echo "This script will re-install helm if it is removed."
        exit 1
    fi
fi

echo "------------------------------------------------------------------------"

echo
echo "* Using $KUBECONFIG for kubeconfig..."
OLD_CONTEXT=$($KUBECTL config current-context 2>/dev/null || /bin/true)

echo
echo "* Running $EKSCTL to create EKS cluster..."
$YQ d $config_file jarvice | $EKSCTL create cluster --config-file -

echo
echo "* Tainting jarvice-system nodes..."
$KUBECTL taint nodes -l node-role.kubernetes.io/jarvice-system=true \
    node-role.kubernetes.io/jarvice-system=true:NoSchedule

#if [ -n "$install_weave_plugin" ]; then
#    echo
#    echo "* Installing Weave Net network plugin into cluster..."
#    $KUBECTL apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
#fi

JARVICE_MACHINES_ADD='[{"mc_name":"n0", "mc_description":"2 core, 16GB RAM (CPU only)", "mc_cores":"2", "mc_slots":"16", "mc_gpus":"0", "mc_ram":"16", "mc_swap":"8", "mc_scratch":"64", "mc_devices":"", "mc_properties":"node-role.kubernetes.io/jarvice-compute=true", "mc_slave_properties":"", "mc_slave_gpus":"0", "mc_slave_ram":"16", "mc_scale_min":"1", "mc_scale_max":"1", "mc_scale_select":"", "mc_lesser":"1", "mc_price":"0.00", "mc_priority":"0", "mc_privs":"", "mc_arch":"x86_64"}'
JARVICE_MACHINES_ADD+=', {"mc_name":"n1", "mc_description":"4 core, 32GB RAM (CPU Only)", "mc_cores":"4", "mc_slots":"16", "mc_gpus":"0", "mc_ram":"32", "mc_swap":"16", "mc_scratch":"100", "mc_devices":"", "mc_properties":"node-role.kubernetes.io/jarvice-compute=true", "mc_slave_properties":"", "mc_slave_gpus":"0", "mc_slave_ram":"32", "mc_scale_min":"1", "mc_scale_max":"1", "mc_scale_select":"", "mc_lesser":"1", "mc_price":"0.00", "mc_priority":"0", "mc_privs":"", "mc_arch":"x86_64"}'
JARVICE_MACHINES_ADD+=', {"mc_name":"n3", "mc_description":"16 core, 128GB RAM (CPU Only)", "mc_cores":"16", "mc_slots":"16", "mc_gpus":"0", "mc_ram":"128", "mc_swap":"64", "mc_scratch":"500", "mc_devices":"", "mc_properties":"node-role.kubernetes.io/jarvice-compute=true", "mc_slave_properties":"", "mc_slave_gpus":"0", "mc_slave_ram":"128", "mc_scale_min":"1", "mc_scale_max":"256", "mc_scale_select":"", "mc_lesser":"1", "mc_price":"0.00", "mc_priority":"0", "mc_privs":"", "mc_arch":"x86_64"}'
if [ -n "$install_nvidia_plugin" ]; then
    kubectl_install_nvidia_plugin
    JARVICE_MACHINES_ADD+=', {"mc_name":"ng0", "mc_description":"2 core, 16GB RAM with Nvidia GPU", "mc_cores":"2", "mc_slots":"16", "mc_gpus":"1", "mc_ram":"16", "mc_swap":"8", "mc_scratch":"64", "mc_devices":"", "mc_properties":"node-role.kubernetes.io/jarvice-compute=true", "mc_slave_properties":"", "mc_slave_gpus":"0", "mc_slave_ram":"16", "mc_scale_min":"1", "mc_scale_max":"1", "mc_scale_select":"", "mc_lesser":"1", "mc_price":"0.00", "mc_priority":"0", "mc_privs":"", "mc_arch":"x86_64"}'
    JARVICE_MACHINES_ADD+=', {"mc_name":"ng1", "mc_description":"4 core, 32GB RAM with Nvidia GPU", "mc_cores":"4", "mc_slots":"16", "mc_gpus":"1", "mc_ram":"32", "mc_swap":"16", "mc_scratch":"100", "mc_devices":"", "mc_properties":"node-role.kubernetes.io/jarvice-compute=true", "mc_slave_properties":"", "mc_slave_gpus":"0", "mc_slave_ram":"32", "mc_scale_min":"1", "mc_scale_max":"1", "mc_scale_select":"", "mc_lesser":"1", "mc_price":"0.00", "mc_priority":"0", "mc_privs":"", "mc_arch":"x86_64"}'
    JARVICE_MACHINES_ADD+=', {"mc_name":"ng3", "mc_description":"16 core, 128GB RAM with Nvidia GPU", "mc_cores":"16", "mc_slots":"16", "mc_gpus":"1", "mc_ram":"128", "mc_swap":"64", "mc_scratch":"500", "mc_devices":"", "mc_properties":"node-role.kubernetes.io/jarvice-compute=true", "mc_slave_properties":"", "mc_slave_gpus":"1", "mc_slave_ram":"128", "mc_scale_min":"1", "mc_scale_max":"2", "mc_scale_select":"", "mc_lesser":"1", "mc_price":"0.00", "mc_priority":"0", "mc_privs":"", "mc_arch":"x86_64"}'
fi
JARVICE_MACHINES_ADD+=']'

HELM2=$($HELM version 2>/dev/null | grep '^Client' || /bin/true)
if [ -n "$HELM2" ]; then
    echo
    echo "* Using Helm ($HELM) v2 client..."
    echo
    echo "* Initializing helm and installing Tiller into cluster..."
    $HELM init

    echo
    echo "* Creating/setting service account and cluster role binding for Tiller..."
    tiller_sa_yaml=$(cat <<EOF
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller
    namespace: kube-system
EOF
)
    echo "$tiller_sa_yaml" | $KUBECTL --namespace kube-system create -f -
    $HELM init --upgrade --service-account tiller
    $KUBECTL -n kube-system patch deployment tiller-deploy \
        -p '{"spec": {"template":{"spec":{"tolerations":[{"key": "node-role.kubernetes.io/jarvice-system", "effect": "NoSchedule", "operator": "Exists"}]}}}}'
    sleep 1

    echo
    echo -n "* "
    $KUBECTL --namespace kube-system rollout status \
        deployment tiller-deploy --watch=true
fi

helm_stable_repo=$($HELM repo list 2>/dev/null | grep ^stable)
if [ -z "$helm_stable_repo" ]; then
    echo
    echo "* Adding helm chart repository for stable..."
    $HELM repo add stable https://kubernetes-charts.storage.googleapis.com/
fi

echo
echo "* Updating helm chart repositories..."
$HELM repo update

classes="jarvice-db jarvice-user"
for class in $classes; do
    echo
    echo "* Creating '$class' StorageClass..."
    storage_class_yaml=$(cat <<EOF
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: $class
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2
reclaimPolicy: Delete
mountOptions:
  - debug
EOF
)
    echo "$storage_class_yaml" | $KUBECTL --namespace kube-system create -f -
done

filters="Name=tag-key,Values=kubernetes.io/cluster/$eks_cluster_name"
filters+=" Name=tag:kubernetes.io/created-for/pvc/namespace,Values=$helm_namespace"
filters+=" Name=tag:kubernetes.io/created-for/pvc/name,Values=jarvice-db-pvc"
filters+=" Name=status,Values=available"
echo
echo "* Searching for 'jarvice-db-pvc' EBS volume previously created for '$eks_cluster_name'..."
pvc_vol=$($AWS --region $aws_region ec2 \
        describe-volumes --no-paginate --output=json --filters $filters)
vol_id=$(echo "$pvc_vol" | $JQ -r ".Volumes[0].VolumeId | select(.!=null)")
if [ -n "$vol_id" ]; then
    vol_size=$(echo "$pvc_vol" | $JQ -r ".Volumes[0].Size")
    vol_zone=$(echo "$pvc_vol" | $JQ -r ".Volumes[0].AvailabilityZone")
    echo "Found ${vol_size}GB volume '$vol_id' in availability zone '$vol_zone'..."
    echo
    echo "* Recreating 'jarvice-db-pv' PersistentVolume using '$vol_id'..."
    pv_yaml=$(cat <<EOF
apiVersion: v1
kind: PersistentVolume
metadata:
  name: jarvice-db-pv
spec:
  awsElasticBlockStore:
    fsType: ext4
    volumeID: aws://$vol_zone/$vol_id
  capacity:
    storage: ${vol_size}Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
  storageClassName: jarvice-db
  mountOptions:
    - debug
EOF
)
    echo "$pv_yaml" | $KUBECTL --namespace kube-system create -f -
else
    echo "No previous EBS volume was found."
fi

jarvice_jobs_domain=$(get_yaml_value $config_file "jarvice.JARVICE_JOBS_DOMAIN")
jarvice_jobs_lb_service=$(get_yaml_value $config_file "jarvice.JARVICE_JOBS_LB_SERVICE")
jarvice_pvc_vault_size=$(get_yaml_value $config_file "jarvice.JARVICE_PVC_VAULT_SIZE")
jarvice_pvc_vault_name=$(get_yaml_value $config_file "jarvice.JARVICE_PVC_VAULT_NAME")
jarvice_pvc_vault_storageclass=$(get_yaml_value $config_file "jarvice.JARVICE_PVC_VAULT_STORAGECLASS")
jarvice_pvc_vault_accessmodes=$(get_yaml_value $config_file "jarvice.JARVICE_PVC_VAULT_ACCESSMODES")
jarvice_machines_add_env="$(echo "$JARVICE_MACHINES_ADD" | sed -e 's#\([]{}",[/]\)#\\\1#g')"
echo
echo "* Running helm to deploy JARVICE into cluster..."
HELM_NAME_CHART="$helm_name $jarvice_chart_dir"
[ -n "$HELM2" ] && HELM_NAME_CHART="$jarvice_chart_dir --name $helm_name"
$KUBECTL create namespace $helm_namespace
$HELM install $HELM_NAME_CHART --namespace $helm_namespace --debug \
    --set jarvice.imagePullSecret="$jarvice_image_pull_secret" \
    --set jarvice.JARVICE_LICENSE_LIC="$jarvice_license" \
    --set jarvice.JARVICE_REMOTE_USER="$jarvice_username" \
    --set jarvice.JARVICE_REMOTE_APIKEY="$jarvice_apikey" \
    --set jarvice.JARVICE_CLUSTER_TYPE="$jarvice_cluster_type" \
    --set jarvice.JARVICE_SCHED_SERVER_KEY="$jarvice_sched_server_key" \
    --set jarvice.JARVICE_JOBS_DOMAIN="$jarvice_jobs_domain" \
    --set jarvice.JARVICE_JOBS_LB_SERVICE="$jarvice_jobs_lb_service" \
    --set jarvice.JARVICE_PVC_VAULT_SIZE="$jarvice_pvc_vault_size" \
    --set jarvice.JARVICE_PVC_VAULT_NAME="$jarvice_pvc_vault_name" \
    --set jarvice.JARVICE_PVC_VAULT_STORAGECLASS="$jarvice_pvc_vault_storageclass" \
    --set jarvice.JARVICE_PVC_VAULT_ACCESSMODES="$jarvice_pvc_vault_accessmodes" \
    --set jarvice_db.persistence.enabled="true" \
    --set jarvice_dal.env.JARVICE_MACHINES_ADD="$jarvice_machines_add_env"
sleep 1

echo
echo "* Installing LXCFS daemonset..."
$KUBECTL --namespace kube-system create \
    -f https://raw.githubusercontent.com/nimbix/lxcfs-initializer/master/lxcfs-daemonset.yaml

echo
echo "* Installing cluster-autoscaler..."
cluster_autoscaler_yaml=$(cat <<EOF
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
  name: cluster-autoscaler
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
  - apiGroups: [""]
    resources: ["events", "endpoints"]
    verbs: ["create", "patch"]
  - apiGroups: [""]
    resources: ["pods/eviction"]
    verbs: ["create"]
  - apiGroups: [""]
    resources: ["pods/status"]
    verbs: ["update"]
  - apiGroups: [""]
    resources: ["endpoints"]
    resourceNames: ["cluster-autoscaler"]
    verbs: ["get", "update"]
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["watch", "list", "get", "update"]
  - apiGroups: [""]
    resources:
      - "pods"
      - "services"
      - "replicationcontrollers"
      - "persistentvolumeclaims"
      - "persistentvolumes"
    verbs: ["watch", "list", "get"]
  - apiGroups: ["extensions"]
    resources: ["replicasets", "daemonsets"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["policy"]
    resources: ["poddisruptionbudgets"]
    verbs: ["watch", "list"]
  - apiGroups: ["apps"]
    resources: ["statefulsets", "replicasets", "daemonsets"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["batch", "extensions"]
    resources: ["jobs"]
    verbs: ["get", "list", "watch", "patch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
  - apiGroups: [""]
    resources: ["configmaps"]
    verbs: ["create","list","watch"]
  - apiGroups: [""]
    resources: ["configmaps"]
    resourceNames: ["cluster-autoscaler-status", "cluster-autoscaler-priority-expander"]
    verbs: ["delete", "get", "update", "watch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler
    namespace: kube-system

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler
    namespace: kube-system

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    app: cluster-autoscaler
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
      annotations:
        prometheus.io/scrape: 'true'
        prometheus.io/port: '8085'
    spec:
      serviceAccountName: cluster-autoscaler
      containers:
        - image: k8s.gcr.io/cluster-autoscaler:v1.12.3
          name: cluster-autoscaler
          resources:
            limits:
              cpu: 100m
              memory: 300Mi
            requests:
              cpu: 100m
              memory: 300Mi
          command:
            - ./cluster-autoscaler
            - --v=4
            - --stderrthreshold=info
            - --cloud-provider=aws
            - --skip-nodes-with-local-storage=false
            - --expander=least-waste
            - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/$eks_cluster_name
          volumeMounts:
            - name: ssl-certs
              mountPath: /etc/ssl/certs/ca-certificates.crt
              readOnly: true
          imagePullPolicy: "Always"
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/jarvice-system
        operator: Exists
      volumes:
        - name: ssl-certs
          hostPath:
            path: "/etc/ssl/certs/ca-bundle.crt"
---
EOF
)
echo "$cluster_autoscaler_yaml" | $KUBECTL --namespace kube-system create -f -

#$HELM install --debug \
#    --set nodeSelector."node-role\.kubernetes\.io/jarvice-system"="true" \
#    --set tolerations[0]."key"="node-role\.kubernetes\.io/jarvice-system" \
#    --set tolerations[0]."effect"="NoSchedule" \
#    --set tolerations[0]."operator"="Exists" \
#    --set rbac.create=true \
#    --set rbac.pspEnabled=true \
#    --set cloudProvider="aws" \
#    --set awsRegion="$aws_region" \
#    --set autoDiscovery.clusterName="$eks_cluster_name" \
#    --namespace kube-system --name cluster-autoscaler stable/cluster-autoscaler

echo
echo "* Installing Traefik ingress controller..."
cert_file=$(get_yaml_value $config_file "jarvice.ingress.cert_file")
key_file=$(get_yaml_value $config_file "jarvice.ingress.key_file")
traefik_lb_ip=$(get_yaml_value $config_file "jarvice.ingress.loadBalancerIP")

[ -n "$cert_file" ] && default_cert="$(cat $cert_file | base64 -w 0)" && \
    default_cert="--set ssl.defaultCert=$default_cert"
[ -n "$key_file" ] && default_key="$(cat $key_file | base64 -w 0)" && \
    default_key="--set ssl.defaultKey=$default_key"
[ -n "$traefik_lb_ip" ] && lb_ip="--set loadBalancerIP=$traefik_lb_ip"

HELM_NAME_CHART="traefik stable/traefik"
[ -n "$HELM2" ] && HELM_NAME_CHART="stable/traefik --name traefik"
$HELM install $HELM_NAME_CHART --namespace kube-system --debug \
    --set nodeSelector."node-role\.kubernetes\.io/jarvice-system"="true" \
    --set tolerations[0]."key"="node-role\.kubernetes\.io/jarvice-system" \
    --set tolerations[0]."effect"="NoSchedule" \
    --set tolerations[0]."operator"="Exists" \
    --set rbac.enabled=true \
    --set ssl.enabled=true \
    --set ssl.enforced=true \
    --set ssl.permanentRedirect=true \
    --set ssl.insecureSkipVerify=true \
    --set replicas=3 \
    --set memoryRequest=1Gi --set memoryLimit=1Gi \
    --set cpuRequest=1 --set cpuLimit=1 \
    $default_cert $default_key $lb_ip

#    --set dashboard.enabled=true \
#    --set dashboard.domain=traefik-dashboard.<domain> \

filters="Name=tag-key,Values=kubernetes.io/cluster/$eks_cluster_name"
filters+=" Name=tag:kubernetes.io/created-for/pvc/namespace,Values=$helm_namespace-jobs"
filters+=" Name=status,Values=available"
echo
echo "* Searching for EBS user vault volumes previously created for '$eks_cluster_name'..."
vol_ids=$($AWS --region $aws_region ec2 \
    describe-volumes --no-paginate --output=json --filters $filters | \
    $JQ -r ".Volumes[].VolumeId | select(.!=null)")
for vol_id in $vol_ids; do
    pvc_vol=$($AWS --region $aws_region ec2 \
        describe-volumes --no-paginate --output=json --volume-ids $vol_id)
    vol_type=$(echo "$pvc_vol" | $JQ -r ".Volumes[0].VolumeType")
    vol_size=$(echo "$pvc_vol" | $JQ -r ".Volumes[0].Size")
    vol_zone=$(echo "$pvc_vol" | $JQ -r ".Volumes[0].AvailabilityZone")
    vol_pv_name=$(echo "$pvc_vol" | $JQ -r ".Volumes[0].Tags[] | select(.Key==\"kubernetes.io/created-for/pvc/name\").Value")
    echo "Found ${vol_size}GB volume '$vol_id' in availability zone '$vol_zone'..."
    echo
    echo "* Recreating PersistentVolume '$vol_pv_name' using '$vol_id' for cluster '$eks_cluster_name'..."
    pv_yaml=$(cat <<EOF
apiVersion: v1
kind: PersistentVolume
metadata:
  name: $vol_pv_name
spec:
  awsElasticBlockStore:
    fsType: ext4
    volumeID: aws://$vol_zone/$vol_id
  capacity:
    storage: ${vol_size}Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
  storageClassName: $vol_type
  mountOptions:
    - debug
EOF
)
    echo "$pv_yaml" | $KUBECTL --namespace kube-system create -f -

    echo
    echo "* Recreating PersistentVolumeClaim '$vol_pv_name' in '$helm_namespace-jobs' namespace..."
    pvc_yaml=$(cat <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: $vol_pv_name
spec:
  resources:
    requests:
      storage: ${vol_size}Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  storageClassName: $vol_type
EOF
)
    echo "$pvc_yaml" | $KUBECTL --namespace $helm_namespace-jobs create -f -
done

if [ -z "$vol_ids" ]; then
    echo "No previous EBS user vault volumes were found."
fi

DEPLOYMENTS="jarvice-dal jarvice-scheduler jarvice-k8s-scheduler"
DEPLOYMENTS+=" jarvice-pod-scheduler jarvice-api jarvice-mc-portal"
DEPLOYMENTS+=" jarvice-appsync"
if [ "$jarvice_cluster_type" = "downstream" ]; then
    DEPLOYMENTS="jarvice-k8s-scheduler jarvice-pod-scheduler"
fi
for deployment in $DEPLOYMENTS; do
    echo
    echo -n "* "
    $KUBECTL --namespace $helm_namespace rollout status \
        deployment $deployment --watch=true
done

echo
echo "------------------------------------------------------------------------"

echo
echo "JARVICE is deployed and ready!"
echo
echo "This EKS cluster may be deleted by executing:"
echo "\$ $0 --eks-cluster-delete --config-file $config_file"

if [ -n "$OLD_CONTEXT" ]; then
    echo
    echo "kubeconfig file ($KUBECONFIG) has been modified."
    echo "Revert to the previous kubeconfig current-context by executing:"
    echo "\$ kubectl config set current-context $OLD_CONTEXT"
fi

echo
echo "* Getting ingress hosts..."
sleep 10

set +e
API_HOST=$($KUBECTL --namespace $helm_namespace \
    describe services jarvice-api-lb 2>/dev/null | \
    grep Ingress | awk '{print $3}')
PORTAL_HOST=$($KUBECTL --namespace $helm_namespace \
    describe services jarvice-mc-portal-lb 2>/dev/null | \
    grep Ingress | awk '{print $3}')
SCHED_HOST=$($KUBECTL --namespace $helm_namespace \
    describe services jarvice-k8s-scheduler-lb 2>/dev/null | \
    grep Ingress | awk '{print $3}')
TRAEFIK_HOST=$($KUBECTL --namespace kube-system \
    describe services traefik 2>/dev/null | \
    grep Ingress | awk '{print $3}')

echo
if [ -n "$API_HOST" ]; then
    echo
    echo "JARVICE API endpoint is available at:"
    echo "https://$API_HOST/"
fi
if [ -n "$PORTAL_HOST" ]; then
    echo
    echo "JARVICE portal is available at:"
    echo "https://$PORTAL_HOST/"
fi
if [ -n "$SCHED_HOST" ]; then
    echo
    echo "JARVICE downstream scheduler endpoint is available at:"
    echo "https://$SCHED_HOST/"
fi

echo
echo "Optionally, set up DNS for Traefik ingress available at:"
echo "$TRAEFIK_HOST"

echo
echo "Use '$EKSCTL get/scale/create/delete nodegroup --cluster $eks_cluster_name' to modify cluster nodes."

echo

